{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:20.165620Z",
     "iopub.status.busy": "2020-11-15T10:04:20.164463Z",
     "iopub.status.idle": "2020-11-15T10:04:28.076120Z",
     "shell.execute_reply": "2020-11-15T10:04:28.076977Z"
    },
    "papermill": {
     "duration": 7.942917,
     "end_time": "2020-11-15T10:04:28.077207",
     "exception": false,
     "start_time": "2020-11-15T10:04:20.134290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version = 2.3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "import random , time, os\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import umap\n",
    "\n",
    "#import for the model\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Activation,Dense,Dropout,BatchNormalization,Input\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "print('TensorFlow version =',tf.__version__)\n",
    "\n",
    "#import preprocessing\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import log_loss, roc_curve\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:28.916764Z",
     "iopub.status.busy": "2020-11-15T10:04:28.915801Z",
     "iopub.status.idle": "2020-11-15T10:04:30.245706Z",
     "shell.execute_reply": "2020-11-15T10:04:30.245133Z"
    },
    "papermill": {
     "duration": 2.149239,
     "end_time": "2020-11-15T10:04:30.245864",
     "exception": false,
     "start_time": "2020-11-15T10:04:28.096625",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURE GPUs\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "gpus = tf.config.list_physical_devices('GPU'); print(gpus)\n",
    "if len(gpus)==1: strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "else: strategy = tf.distribute.MirroredStrategy()\n",
    "    \n",
    "#Function to convert in tensor\n",
    "def my_func(arg):\n",
    "    arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n",
    "    return arg\n",
    "    \n",
    "# Function to seed everything\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017874,
     "end_time": "2020-11-15T10:04:30.284042",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.266168",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocess the data with features and embedding with umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:30.362195Z",
     "iopub.status.busy": "2020-11-15T10:04:30.336018Z",
     "iopub.status.idle": "2020-11-15T10:04:30.373937Z",
     "shell.execute_reply": "2020-11-15T10:04:30.373375Z"
    },
    "papermill": {
     "duration": 0.071399,
     "end_time": "2020-11-15T10:04:30.374055",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.302656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#remove the ctl vehicle with no moa, map type and dose \n",
    "\n",
    "def mapping_and_filter(train, train_targets, test):\n",
    "    cp_type = {'trt_cp': 0, 'ctl_vehicle': 1}\n",
    "    cp_dose = {'D1': 0, 'D2': 1}\n",
    "    for df in [train, test]:\n",
    "        df['cp_type'] = df['cp_type'].map(cp_type)\n",
    "        df['cp_dose'] = df['cp_dose'].map(cp_dose)\n",
    "    train_targets = train_targets[train['cp_type'] == 0].reset_index(drop = True)\n",
    "    train = train[train['cp_type'] == 0].reset_index(drop = True)\n",
    "    train_targets.drop(['sig_id'], inplace = True, axis = 1)\n",
    "    train.drop(['sig_id'], inplace = True, axis = 1)\n",
    "    test.drop(['sig_id'], inplace = True, axis = 1)\n",
    "    return train, train_targets, test\n",
    "\n",
    "# Function to scale our data\n",
    "def scaling(train, test):\n",
    "    features = train.columns[3:]\n",
    "    train.columns[3:]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n",
    "    train[features] = scaler.transform(train[features])\n",
    "    test[features] = scaler.transform(test[features])\n",
    "    return train, test, features\n",
    "\n",
    "\n",
    "# Function to extract common stats features\n",
    "def fct_stats(train, test):\n",
    "    \n",
    "    features_g = list([col for col in train.columns if col.startswith('g-')])#train.columns[4:140])\n",
    "    features_c = list([col for col in train.columns if col.startswith('c-')])#train.columns[140:244])\n",
    "    \n",
    "    for df in [train, test]:\n",
    "        df['g_sum'] = df[features_g].sum(axis = 1)\n",
    "        df['g_mean'] = df[features_g].mean(axis = 1)\n",
    "        df['g_std'] = df[features_g].std(axis = 1)\n",
    "        df['g_kurt'] = df[features_g].kurtosis(axis = 1)\n",
    "        df['g_skew'] = df[features_g].skew(axis = 1)\n",
    "        df['c_sum'] = df[features_c].sum(axis = 1)\n",
    "        df['c_mean'] = df[features_c].mean(axis = 1)\n",
    "        df['c_std'] = df[features_c].std(axis = 1)\n",
    "        df['c_kurt'] = df[features_c].kurtosis(axis = 1)\n",
    "        df['c_skew'] = df[features_c].skew(axis = 1)\n",
    "        df['gc_sum'] = df[features_g + features_c].sum(axis = 1)\n",
    "        df['gc_mean'] = df[features_g + features_c].mean(axis = 1)\n",
    "        df['gc_std'] = df[features_g + features_c].std(axis = 1)\n",
    "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis = 1)\n",
    "        df['gc_skew'] = df[features_g + features_c].skew(axis = 1)\n",
    "        \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def umap_embedding(train,test):\n",
    "    \n",
    "    feat_g = list([col for col in train.columns if col.startswith('g-')])#train.columns[4:140])\n",
    "    feat_c = list([col for col in train.columns if col.startswith('c-')])#train.columns[140:244])\n",
    "    \n",
    "    Umap_g = umap.UMAP(n_neighbors =10, random_state = 34, metric='cosine').fit(train[feat_g])\n",
    "    Umap_c = umap.UMAP( n_neighbors =10, random_state = 34, metric='cosine').fit(train[feat_c])\n",
    "\n",
    "    umap_g = pd.DataFrame(Umap_g.embedding_)\n",
    "    umap_g.columns = ['X_umap_G', 'Y_umap_G']\n",
    "    umap_c = pd.DataFrame(Umap_c.embedding_)\n",
    "    umap_c.columns = ['X_umap_C', 'Y_umap_C']\n",
    "    train = pd.concat([train.reset_index(drop=True), umap_g.reset_index(drop=True), umap_c.reset_index(drop=True)], axis=1, sort=False)\n",
    "\n",
    "    Uma_g = umap.UMAP(n_neighbors =10, random_state=34, metric='cosine').fit(test[feat_g])\n",
    "    Uma_c = umap.UMAP(n_neighbors =10, random_state=34, metric='cosine').fit(test[feat_c])                                                \n",
    "                                                     \n",
    "    uma_g = pd.DataFrame(Uma_g.embedding_)\n",
    "    uma_g.columns = ['X_umap_G', 'Y_umap_G']\n",
    "    uma_c = pd.DataFrame(Uma_c.embedding_)\n",
    "    uma_c.columns = ['X_umap_C', 'Y_umap_C']\n",
    "    test = pd.concat([test.reset_index(drop=True), uma_g.reset_index(drop=True), uma_c.reset_index(drop=True)], axis=1, sort=False)\n",
    "  \n",
    "    return train,test\n",
    "    \n",
    "\n",
    "# Function to calculate the mean log loss of the targets including clipping\n",
    "def mean_log_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    metrics = []\n",
    "    for target in range(206):\n",
    "        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n",
    "    return np.mean(metrics)\n",
    "\n",
    "#find the weights for initialise the model in using log(TP/TN)\n",
    "def initiale_weights(train_targets):\n",
    "    tp = train_targets.sum(axis=0)\n",
    "    tp = np.array([tp], dtype = np.int)\n",
    "    res=[]\n",
    "    for i in tp:\n",
    "        res=np.around([(i/23814)*100])\n",
    "    pos = np.count_nonzero(res >= 1)\n",
    "    neg =np.count_nonzero(res == 0)\n",
    "    \n",
    "    initial_bias = np.log([pos/neg])\n",
    "    \n",
    "    return initial_bias, neg, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:30.428551Z",
     "iopub.status.busy": "2020-11-15T10:04:30.426605Z",
     "iopub.status.idle": "2020-11-15T10:04:30.429397Z",
     "shell.execute_reply": "2020-11-15T10:04:30.429943Z"
    },
    "papermill": {
     "duration": 0.036869,
     "end_time": "2020-11-15T10:04:30.430076",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.393207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to extract pca features\n",
    "def fe_pca(train, test, n_components_g = 70, n_components_c = 10, SEED = 123):\n",
    "    \n",
    "    features_g = list([col for col in train.columns if col.startswith('g-')])\n",
    "    features_c = list([col for col in train.columns if col.startswith('c-')])\n",
    "    \n",
    "    def create_pca(train, test, features, kind = 'g', n_components = n_components_g):\n",
    "        train_ = train[features].copy()\n",
    "        test_ = test[features].copy()\n",
    "        data = pd.concat([train_, test_], axis = 0)\n",
    "        pca = PCA(n_components = n_components,  random_state = SEED)\n",
    "        data = pca.fit_transform(data)\n",
    "        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n",
    "        data = pd.DataFrame(data, columns = columns)\n",
    "        train_ = data.iloc[:train.shape[0]]\n",
    "        test_ = data.iloc[train.shape[0]:].reset_index(drop = True)\n",
    "        train = pd.concat([train, train_], axis = 1)\n",
    "        test = pd.concat([test, test_], axis = 1)\n",
    "        return train, test\n",
    "    \n",
    "    train, test = create_pca(train, test, features_g, kind = 'g', n_components = n_components_g)\n",
    "    train, test = create_pca(train, test, features_c, kind = 'c', n_components = n_components_c)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:30.487213Z",
     "iopub.status.busy": "2020-11-15T10:04:30.485377Z",
     "iopub.status.idle": "2020-11-15T10:04:30.488172Z",
     "shell.execute_reply": "2020-11-15T10:04:30.488703Z"
    },
    "papermill": {
     "duration": 0.040491,
     "end_time": "2020-11-15T10:04:30.488870",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.448379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Top_feat1 = ['g-139',  'g-26', 'g-149', 'g-744', 'g-735', 'g-532', 'g-571', 'g-311', 'g-52', 'g-50',\n",
    "            'g-476', 'g-22', 'g-599', 'g-47', 'g-321', 'g-555', 'g-580',  'g-639', 'g-487', 'g-12', \n",
    "            'g-718', 'g-439', 'g-15', 'g-635', 'g-765', 'g-356', 'g-759', 'g-253', 'g-235', 'g-72', 'g-570', \n",
    "            'g-162', 'g-200', 'g-587',  'g-43', 'g-563', 'g-83', 'g-44', 'g-335', 'g-634', 'g-749', 'g-99', \n",
    "            'g-300', 'g-180', 'g-519', 'g-34', 'g-312', 'g-615', 'g-273', 'g-265', 'g-606', 'g-165', 'g-30', 'g-148',\n",
    "            'g-4', 'g-211', 'g-65', 'g-345', 'g-127', 'g-280', 'g-740', 'g-574', 'g-479', 'g-45', 'g-533', 'g-203', 'g-537',\n",
    "            'g-194', 'g-761', 'g-156', 'g-651', 'g-750', 'g-309', 'g-590', 'g-9', 'g-531', 'g-432', 'g-81', 'g-208', \n",
    "            'g-586', 'g-108', 'g-175', 'g-192', 'g-337', 'g-202', 'g-178', 'g-655', 'g-205', 'g-407', 'g-89', 'g-617',\n",
    "            'g-706', 'g-69', 'g-161', 'g-79', 'g-3', 'g-627', 'g-282', 'g-252', 'g-260', 'g-437', 'g-610', 'g-576',\n",
    "            'g-488', 'g-324', 'g-584', 'g-370', 'g-350', 'g-664', 'g-422', 'g-640',  'g-387', 'g-117', 'g-696',\n",
    "            'g-140', 'g-206', 'g-769', 'g-318', 'g-546', 'g-466', 'g-231', 'g-342', 'g-418', 'g-701', 'g-20', \n",
    "            'g-18', 'g-463', 'g-591', 'g-100', 'g-157', 'g-357', 'g-392', 'g-122', 'g-91', 'g-158', 'g-628', \n",
    "            'g-90', 'g-763', 'g-351', 'g-322','c-0', 'c-1', 'c-2', 'c-3', 'c-4', 'c-5', 'c-6', 'c-7', 'c-8', 'c-9', \n",
    "            'c-10', 'c-11', 'c-12', 'c-13', 'c-14', 'c-15', 'c-16', 'c-17', 'c-18', 'c-19', 'c-20', 'c-21', 'c-22', \n",
    "            'c-23', 'c-24', 'c-25', 'c-26', 'c-27', 'c-28', 'c-29', 'c-30', 'c-31', 'c-32', 'c-33', 'c-34', 'c-35', \n",
    "            'c-36', 'c-37', 'c-38', 'c-39', 'c-40', 'c-41', 'c-42', 'c-43', 'c-44', 'c-45', 'c-46', 'c-47', 'c-48', \n",
    "            'c-49', 'c-50', 'c-51', 'c-52', 'c-53', 'c-54', 'c-55', 'c-56', 'c-57', 'c-58', 'c-59', 'c-60', 'c-61', \n",
    "            'c-62', 'c-63', 'c-64', 'c-65', 'c-66', 'c-67', 'c-68', 'c-69', 'c-70', 'c-71', 'c-72', 'c-73', 'c-74', \n",
    "            'c-75', 'c-76', 'c-77', 'c-78', 'c-79', 'c-80', 'c-81', 'c-82', 'c-83', 'c-84', 'c-85', 'c-86', 'c-87', \n",
    "            'c-88', 'c-89', 'c-90', 'c-91', 'c-92', 'c-93', 'c-94', 'c-95', 'c-96', 'c-97', 'c-98', 'c-99']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:30.540368Z",
     "iopub.status.busy": "2020-11-15T10:04:30.539360Z",
     "iopub.status.idle": "2020-11-15T10:04:30.542989Z",
     "shell.execute_reply": "2020-11-15T10:04:30.542402Z"
    },
    "papermill": {
     "duration": 0.035305,
     "end_time": "2020-11-15T10:04:30.543093",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.507788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Top_feat = ['g-139', 'c-30', 'g-26', 'g-149', 'g-744', 'g-735', 'g-532', 'g-571', 'g-311', 'g-52', 'g-50',\n",
    "            'g-476', 'g-22', 'g-599', 'g-47', 'g-321', 'g-555', 'g-580', 'c-86', 'g-639', 'g-487', 'g-12', \n",
    "            'g-718', 'g-439', 'g-15', 'g-635', 'g-765', 'g-356', 'g-759', 'g-253', 'g-235', 'g-72', 'g-570', \n",
    "            'g-162', 'g-200', 'g-587', 'c-46', 'g-43', 'g-563', 'g-83', 'g-44', 'g-335', 'g-634', 'g-749', 'g-99', \n",
    "            'g-300', 'g-180', 'g-519', 'g-34', 'g-312', 'g-615', 'g-273', 'g-265', 'g-606', 'g-165', 'g-30', 'g-148',\n",
    "            'g-4', 'g-211', 'g-65', 'g-345', 'g-127', 'g-280', 'g-740', 'g-574', 'g-479', 'g-45', 'g-533', 'g-203', 'g-537',\n",
    "            'g-194', 'g-761', 'g-156', 'g-651', 'g-750', 'g-309', 'g-590', 'g-9', 'g-531', 'g-432', 'g-81', 'g-208', \n",
    "            'g-586', 'g-108', 'g-175', 'g-192', 'g-337', 'g-202', 'g-178', 'g-655', 'g-205', 'g-407', 'g-89', 'g-617',\n",
    "            'g-706', 'g-69', 'g-161', 'g-79', 'g-3', 'g-627', 'g-282', 'g-252', 'g-260', 'g-437', 'g-610', 'g-576',\n",
    "            'g-488', 'g-324', 'g-584', 'g-370', 'g-350', 'g-664', 'g-422', 'g-640', 'c-78', 'g-387', 'g-117', 'g-696',\n",
    "            'g-140', 'g-206', 'g-769', 'g-318', 'c-98', 'g-546', 'g-466', 'g-231', 'g-342', 'g-418', 'g-701', 'g-20', \n",
    "            'c-65', 'g-18', 'g-463', 'g-591', 'g-100', 'g-157', 'g-357', 'g-392', 'g-122', 'g-91', 'g-158', 'g-628', \n",
    "            'g-90', 'g-763', 'g-351', 'g-322']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02321,
     "end_time": "2020-11-15T10:04:30.589410",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.566200",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define the MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:30.647473Z",
     "iopub.status.busy": "2020-11-15T10:04:30.646597Z",
     "iopub.status.idle": "2020-11-15T10:04:30.650832Z",
     "shell.execute_reply": "2020-11-15T10:04:30.650260Z"
    },
    "papermill": {
     "duration": 0.042382,
     "end_time": "2020-11-15T10:04:30.650937",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.608555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_3l(shape):\n",
    "    \n",
    "    output_bias = tf.keras.initializers.Constant(-1.65678403)\n",
    "    model = Sequential([\n",
    "    Input(shape),\n",
    "    layers.BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(2048, activation=\"elu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    Dense(1024, activation=\"elu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    Dense(512, activation=\"elu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    Dense(206, activation =\"sigmoid\",bias_initializer = output_bias)])\n",
    "    \n",
    "#define otpimizer   \n",
    "    opt = tf.optimizers.Adam(learning_rate = 0.001)\n",
    "    #opt = tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10)\n",
    "    \n",
    "\n",
    "#define the loss\n",
    "    #LOSS = tf.keras.losses.categorical_crossentropy( label_smoothing=0.0020)\n",
    "    LOSS = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015)\n",
    "    \n",
    "    model.compile( optimizer= opt, loss= LOSS,metrics= tf.keras.metrics.AUC(name='auc'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def build_model_4l(shape):\n",
    "    \n",
    "    output_bias = tf.keras.initializers.Constant(-1.65678403)\n",
    "    model = Sequential([\n",
    "    Input(shape),\n",
    "    layers.BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(2048, activation=\"elu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    Dense(1592, activation=\"elu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    Dense(1024, activation=\"elu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.4),\n",
    "    Dense(512, activation=\"elu\"),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.5),\n",
    "    Dense(206, activation =\"sigmoid\",bias_initializer = output_bias)])\n",
    "    \n",
    "#define otpimizer   \n",
    "    opt = tf.optimizers.Adam(learning_rate = 0.001)\n",
    "    #opt = tfa.optimizers.Lookahead(tf.optimizers.Adam(), sync_period=10)\n",
    "    \n",
    "\n",
    "#define the loss\n",
    "    #LOSS = tf.keras.losses.categorical_crossentropy( label_smoothing=0.0020)\n",
    "    LOSS = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015)\n",
    "    \n",
    "    model.compile( optimizer= opt, loss= LOSS,metrics= tf.keras.metrics.AUC(name='auc'))\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019599,
     "end_time": "2020-11-15T10:04:30.689704",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.670105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Custom the learning schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:30.740720Z",
     "iopub.status.busy": "2020-11-15T10:04:30.739936Z",
     "iopub.status.idle": "2020-11-15T10:04:30.944910Z",
     "shell.execute_reply": "2020-11-15T10:04:30.943980Z"
    },
    "papermill": {
     "duration": 0.235501,
     "end_time": "2020-11-15T10:04:30.945026",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.709525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEcCAYAAAAV2MmlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxcdZnv8c833el00tkXQhYwAYMDCAhGwN0RF4JK0Lk4qHMF9Q7DiFeH6zIw6hVUZlAZHXUQ5CoC1xmRiwsZRRHBDRQkuLAKRAgQEiBJd7Zu0kv6uX+cU0lRdLpPkjrndKq+79erX1V11t/JUk//tueniMDMzCwPY8ougJmZNS4HGTMzy42DjJmZ5cZBxszMcuMgY2ZmuXGQMTOz3DjI2KghKTL8rNzDe5yWXmfBbpx7+Z7ef3dJOknSLyU9JelpSY9I+r6k43fxOpdLWpVXOavu83NJP9/Nc0PSufUtkZWltewCmFV5cc3n7wF/BM6t2ta7h/f4YXqfNbtx7qeAL+7h/XeZpPen970M+BzQDRwIvAF4NfDjostklpWDjI0aEXFr9WdJvcC62u01x7QAioiBjPdYC6zdzfL9eXfOq4MPAd+PiPdUbbsJ+D+S3Bpho5r/gdpeJW1KOV/S2ZIeBvqAwyS1S/qCpLslbZH0hKT/kvQXNec/q7lM0kpJ35R0iqT7JHVLWi7pZTXnPqO5TNKC9Fp/J+mTktZI2pDed37NuRMkXSxpvaTNkr4n6SXp+aeN8NjTgSeG2hERgzX3WSjp/6bP3yvpIUnPqn1JOlLSryT1SHpQ0hlDHLNQ0n9IWpte6w+S3jzEcadI+lN6zD07OWbIZkpJ50oaMe2IpCMkLZPUlTYX3iLp5SOdZ+VzkLG90WkkTUUfSl9XA+OAScCn021/D7QDt0raN8M1Xw58EPg48NdAC/ADSVMznHsO8Fzg3cAHSJrj/qPmmEvT/RcCbwHuH+KYnfktcKqkD0s6aGcHSVqYHvsK4BPAEuA8YGbNoZOB/wS+CSwFbgculvSXVdfaD7gNOAI4CzgR+B3wHUknVh33mvRaD6bP9TmSpr3nZXy2EUk6Cvg1SbD9W+CvgPXATyW9sF73sZxEhH/8Myp/gJXAN2u2BUlQGT/CuS3ABGAzcFbV9tPSayyouU8XMK1q2+L0uLdXbbscWFn1eUF6zC9q7v2hdPvc9PPzgEHgIzXHfSk97rQRnuUg4M702ADWAd8CXldz3JXAlsp9d3Kty9Nr/GXVtnHpNS+t2vZ1kmbFGTXn3wD8oerzLcC9wJiqbcek9/j5cH/u6fZzk6+hZ/0dn1v1+UbgPqCt5u/3PpJmxNL/rfpn5z+uydje6McR8XTtRklvlXSbpA3AAEkH+USy/Vb9m4joqvp8V/q6f4Zzf1jzufbcYwAB/6/muGsyXJuIeAA4EnglcD7wB+DNwPWSPlZ16OuAH0TE6hEu2RMRP6u6fi9JTaT6WY8HrgM2Smqt/ADXA0dImpz2h70IuCaqmu0i4jaSwL3HJI0nee7/BwxWlUPAT0lqbTaKuePf9kbPGhkm6U3At4ErSJqI1pHUHq4jaTYbSWf1h4jolcTunMuOEXCVc+ekr0/VHPdkhmtXyrMN+GX6g6S5JKPKPiHpojRAzgCyDE/uGmJbL8981n2Ad6Y/Q5kBjAfGMvRzZH62EUwnqbV8PP15FkljoqZvykYPBxnbGw3VUXwKsCIiTqtskDSW5EuqbJWguA/wcNX22bt7wYhYLelrJP0fi0j6YtYB83b3mjXWA78CPrOT/atJaov9DP0cs4FHqj5vTV/bao6bMUI5NpD8snARSXPgszjAjG4OMtYoJpB86VX77yS/BZftNpLAeDLw2artJ2c5WdJ+EfHYELsqI+cqI89+ArxF0pyI2J15QNV+TDKA4Z6hmiarynY78N8knVv5spd0DEl/VXWQqbx/PvBAelwrSRPfTkVEt6RfkQxA+J0Dyt7HQcYaxY+BkyR9AfgB8ELg/SS/CZcqIu6X9J/Ap9J5LXeQTKJ8U3rISF+cd0v6Gcnk1IdJRoedAJwBXB0Rj6bHfYJkZN2vJf0zsIKkZnN8RPzNLhb7f5PUjn4p6d9J+limkQSJAyLi3VX3/AnwfUlfBWaRNFfWDrm+Hfgz8Ln0z6AXeC/JoIOR/C+SZsLrJX2dpGY4EzgKaImIs3fx2axA7vi3RvF/SDrF/xr4L5Iv2zcBG8ssVJXTSWbsf4QkWBwKnJnuG6mM/0jyf/WTJF/o3yapZZxNUlsDICJWkgwyuBX4F5LA+0l2Y/JpGrgWk2Rc+GeSUWUXk3TC31R13E+Bd5AMrvgu8GHgH0iGaFdfb4BkuPRjJCPcLkqveXmGsvyOZIDBepIReT8haSY8jLSPykYvRXj5ZbMySPowSZ/HgqraiFlDcXOZWQEkvZGkqekPJM1jLyeZT3O1A4w1MgcZs2JsBk4iaeLqAB4nafr5RJmFMsubm8vMzCw37vg3M7PcuLmsysyZM2PBggVlF8PMbK9yxx13rIuIWUPtc5CpsmDBApYvX152MczM9iqSHtnZPjeXmZlZbhxkzMwsNw4yZmaWGwcZMzPLjYOMmZnlptAgI+l4SfdLWiHpWZlTlfhSuv/OdG3vYc+VdLKkeyQNSlpcc71z0uPvl/T6fJ/OzMxqFRZk0qVaLwKWAIcAb5N0SM1hS0gWYFpEkrX24gzn3g28hZpsrOn+U0iy3R4PfCW9jpmZFaTIeTJHk6xc+BCApKtIUn/fW3XMUuDKSHLd3CppqqQ5JAsgDXluRNyXbqu931LgqnT98oclrUjL8Jucnm+3/eKBtdyxsnYF3/yMG9vCO1/8HCa1jy3snmbWnIoMMvNI1pKoWEWy9sVIx8zLeO5Q97t1iGs9g6TTSWpN7L///iNcMh/nLruHh9d18+w4WX+VVHX7TZ/AiUfMzf+GZtbUigwyQ32F1mbn3NkxWc7dnfsREZcClwIsXry4lGyh67b0ctpLFnDuiYfmfq+1m3t50fk/ZePT/bnfy8ysyCCzCtiv6vN8YHXGY9oynLs79ytd/7ZBNm8dYNqEtkLuN6k9+Svf5CBjZgUocnTZ7cAiSQsltZF0yi+rOWYZ8M50lNmxwMaIWJPx3FrLgFMkjZO0kGQwwW/r+UD10NXTB8D0jmL6R9rHttDWOoZNWx1kzCx/hdVkImJA0vuA64EW4LKIuEfSGen+S4DrgBOAFUAP8K7hzgWQ9Gbgy8As4IeS/hARr0+vfTXJwIIB4MyI2FbU82a1oSf5sp/WUUxNBmBy+1g2PT1Q2P3MrHkVmoU5Iq4jCSTV2y6peh/AmVnPTbd/D/jeTs45Hzh/D4qcu87utCZTUHMZwOTxra7JmFkhPOO/ZF1pkCm+JuMgY2b5c5ApWef2PpkiazJj2bTVzWVmlj8HmZJVajJTJxQ3MXJyeyubXZMxswI4yJSss7ufieNaGddaXMabpCbjIGNm+XOQKVlXTx/TChq+XOHRZWZWFAeZknV29xU6sgyS0WV92wbZ2j/qRnSbWYNxkClZV08fU4sOMmliTDeZmVneHGRK1tndV+jIMqhOLeMmMzPLl4NMybq6+wrLW1YxebxrMmZWDAeZEvUObKO7b1thecsqtjeXeRizmeXMQaZEZeQtA5gyPm0u84RMM8uZg0yJyshbBq7JmFlxHGRKVEbeMnCfjJkVx0GmRGXkLQMY1zqGtpYxHl1mZrlzkCnR9ppMwc1lkpzu38wK4SBTos7u5Eu+yOSYFU73b2ZFcJApUVdPH5PaWxnbUvxfw6TxY9ns0WVmljMHmRKVMdu/YnK7m8vMLH8OMiXq6il+tn/F5PFuLjOz/DnIlKj8moyby8wsXw4yJdrQ019eTcYd/2ZWAAeZEiU1meJHlkHSXNY74DVlzCxfDjIlebpvG0/3byt8tn/F5DTdv0eYmVmeHGRK0tVTTt6yCqeWMbMiOMiUpLOkvGUVTpJpZkVwkClJV0l5yyomO92/mRXAQaYk22syJaSUAddkzKwYDjIlKSs5ZoX7ZMysCA4yJens6UeCKePLrcl4dJmZ5clBpiRd3X1MGT+W1hKSYwK0jx3D2Ba5uczMcuUgU5Kunr7Shi9DsqbMpPaxbi4zs1wVGmQkHS/pfkkrJJ09xH5J+lK6/05JR410rqTpkm6Q9GD6Oi3dPlbSFZLuknSfpHOKecpsunr6Shu+XDG5vdWrY5pZrgoLMpJagIuAJcAhwNskHVJz2BJgUfpzOnBxhnPPBm6MiEXAjelngJOBcRFxGPBC4O8kLcjl4XZDZ3d5ecsqJo93TcbM8lVkTeZoYEVEPBQRfcBVwNKaY5YCV0biVmCqpDkjnLsUuCJ9fwVwUvo+gA5JrcB4oA/YlNOz7bKu7r7Shi9XOEmmmeWtyCAzD3is6vOqdFuWY4Y7d3ZErAFIX/dJt18DdANrgEeBCyOis7ZQkk6XtFzS8rVr1+7Oc+2yiKCzp4/pE8uuyTjdv5nlq8ggoyG2RcZjspxb62hgGzAXWAh8UNIBz7pIxKURsTgiFs+aNWuES9ZHT982+gYGS+34B9dkzCx/RQaZVcB+VZ/nA6szHjPcuU+mTWqkr0+l298O/Dgi+iPiKeAWYHEdnmOPdZY8EbPCfTJmlrcig8ztwCJJCyW1AacAy2qOWQa8Mx1ldiywMW0CG+7cZcCp6ftTgWvT948Cr06v1QEcC/wpr4fbFZW8ZaNhdNnW/kF6B7ymjJnlo7WoG0XEgKT3AdcDLcBlEXGPpDPS/ZcA1wEnACuAHuBdw52bXvoC4GpJ7yEJLCen2y8CvgHcTdLc9o2IuDP/Jx1ZpSZT1oJlFZXUMpu3DjBuYkupZTGzxlRYkAGIiOtIAkn1tkuq3gdwZtZz0+3rgeOG2L6FHQFnVNlekym7uawqtczMieNKLYuZNSbP+C9BZ3fSD1JWmv+K7en+3flvZjlxkCnBhp4+xmhHTaIs29P9u/PfzHJSaHOZJTq7+5g2oY0xY4YamV2cSWmQWbell6f7iun8H9ui0pKCmlnxHGRKMBrylsGOBdPO+vYfOYs/FnLPGR1t3PyPr2Z8mwcamDUDB5kSdHaXm4G5Yp/J7Xzhr4/gyU29hdzv3tWbWPbH1TyxaSsLZ3YUck8zK5eDTAm6uvtZMHNC2cUA4M1Hzi/sXj+//ymW/XE1nd29DjJmTcKN4yXo7OkrfWRZGWZ0JMOk12/pK7kkZlYUB5mCRQRd3X1MHQXNZUWbkSYEXd/tIGPWLBxkCra5d4CBwRgVfTJFq9TeOh1kzJqGg0zBurpHR96yMrSPbaGjrcXNZWZNJHOQkTRb0ockXSxpZrrtpZIW5le8xjNa8paVZfrENjq7ixnNZmblyxRkJL0QuB94B/AeYHK667XA+fkUrTGNlrxlZZnRMc59MmZNJGtN5kLgixFxJFD9a+j1wEvrXqoG1jVK8paVZUZHm5vLzJpI1iDzQuCKIbavAWbXrziNb7SsJVOW6R1t7vg3ayJZg8zTwLQhtv8FO1aitAw6u/toHSMmjWvOebBJn0wfyaoOZtbosgaZa4FPSKosOhKSFgCfAb6TQ7kaViVvmVRucsyyzOhoo2/bIFt6B8ouipkVIGuQ+RAwHVgLTABuJlm9cgPwsXyK1phGS96ysnjWv1lzydRmExGbgJdJejVwFElw+l1E/DTPwjWiru5+pjXp8GVImssgmfW/wPnLzBpepiAj6Z3AtyPiJuCmqu1twCkRcWVO5Ws4nT19LNpnYtnFKM0Mz/o3aypZm8u+AUwZYvukdJ9l1NU9OtaSKcuO1DKekGnWDLIGGQFDDQfaH9hYv+I0tsHBoKvHfTIA69wnY9YUhm0uk3QXSXAJ4BeSqocEtQDPAa7Lr3iNZdPWfgajeefIAIxva2FCW4uby8yaxEh9Mtekr88HfghsqdrXB6zEQ5gz6+qpzPZv3o5/8IRMs2YybJCJiPMAJK0k6fjfWkShGlXli7VZ85ZVzOhoc/4ysyaRdQjzUCllbBd1bc/A3ORBZuI4ntzk31fMmkHWLMxtks6T9ICkrZK2Vf/kXchG0dnkGZgr3Fxm1jyyji77FHAq8K/AIPBh4CJgPfDefIrWeFyTSVSay5y/zKzxZQ0ybwXOiIivAtuAayPi/cAnSNaUsQw6e/poax3DhLaWsotSqukdbfQNDNLd50qwWaPLGmRmA/em77cAU9P3PwZeV+9CNaqu7j6mTRjbtMkxK7ZPyPRcGbOGlzXIPArMTd+vAF6fvn8xyTIAlkFnd3/T98cAzJyYTsj0rH+zhpc1yHwPOC59/0XgPEkPA5cDX8t6M0nHS7pf0gpJZw+xX5K+lO6/U9JRI50rabqkGyQ9mL5Oq9p3uKTfSLpH0l2S2rOWNQ9dPX1N3x8DrsmYNZNMQSYizomI89P31wAvA74MvCUiPprlGpJaSAYLLAEOAd4m6ZCaw5YAi9Kf04GLM5x7NnBjRCwCbkw/I6kV+CZJX9KhwKuA/ixlzUuz5y2rmO4kmWZNY8QgI2mspG9LOrCyLSJui4jPR8QPduFeRwMrIuKhiOgDrgKW1hyzFLgyErcCUyXNGeHcpexYGvoK4KT0/euAOyPij2mZ10dEqT3NnU2et6xiRlW6fzNrbCMGmYjoJ/nC3tPxpvOAx6o+r0q3ZTlmuHNnR8SatKxrgH3S7QeRrOB5vaTfSfrIUIWSdLqk5ZKWr127djceK5ttg8HGp/uZNqG5U8oATGhrZfzYFtZvcZ+MWaPL2ifzXeAte3ivoYZU1QaunR2T5dxarSTNeu9IX98s6bjagyLi0ohYHBGLZ82aNcIld9/Gp/uJ8ByZCk/INGsOmdLKkIwu+5iklwPLge7qnRHx+QzXWAXsV/V5PrA64zFtw5z7pKQ5EbEmbVp7qupav4iIdQCSriNZ1fPGDGWtu+15yxxkgKTJzM1lZo0va03mNKALOBx4N/A/q37el/EatwOLJC2srKgJLKs5ZhnwznSU2bHAxrQJbLhzl5FkIyB9vTZ9fz1wuKQJ6SCAV7Jjrk/huno827+aazJmzSFrgsyFe3qjiBiQ9D6SL/8W4LKIuEfSGen+S0jWpjmBZC5OD/Cu4c5NL30BcLWk95DUuE5Oz+mS9HmSABXAdRHxwz19jt3lDMzPNL2jjQef3DLygWa2V8vaXFYXEXEdNYucpcGl8j6AM7Oem25fz445PLX7vkkyjLl0zlv2TDMnjmPdll4ioukzIJg1sqzNZbaHnIH5maZ3tNE7MEiP85eZNbRCazLNrKu7j/axYxjf5MkxK2akNbpj/vlGiqrIHH/ovnzu5COKuZmZAQ4yhens7vdEzCqvOXg2Z7zyQHoHiqnJ/ObP6/nFA/nNgzKzoTnIFKSrxyllqk3raOPsJX9R2P3+7acP8MUbH6RvYJC2VrcSmxUlU5CRtP9OdgWwNSL8K+IIOrudHLNMc6a0EwFPbd7K/GkTyi6OWdPIWpNZyTAz7CVtAr4BfCQiBupQroazoaeP/af7y60s+04ZD8ATGx1kzIqUNci8DfgscAlwW7rtGJJMyeeSLGL2MWAzyWqZVsM1mXLNnZKs8rB649aSS2LWXLIGmb8HzoqI71Ztu0nS/cAHIuKVkp4CzsNB5ln6tw2yaeuAhy+XaN80yDyx0WvsmRUpaw/oMcBdQ2y/G3hR+v43JDnFrMaGnmQZm+kdzsBclkntY5k4rpU1rsmYFSprkHmEpGms1t+SpHIBmAV01qNQjaaSt8yjy8o1Z0o7TzjImBUqa3PZB4HvSDqBHbnAXgQcCPxVesyLgKvrXsIGUMlb5nky5dp3Srv7ZMwKljVB5g8lLQLeCzyPZH2XZcAlEfFoesxXcivlXq6St2yqg0yp5kxp54EnPdrerEiZJ2NGxGPAOTmWpWF1Os3/qLDvlPE8tbmX/m2DjG3xhEyzImQOMpImAC8gWd74Gf9Da0adWY0dNRl3/Jdpbjohc+3mXuZOHV92ccyaQtYZ/68BvgXMGGJ3kKzxYjvR2d1PR1sL7WP9x1SmyjDmNRufdpAxK0jWNoMvAj8E5kfEmJoff3OOYIPzlo0Kc9JZ/x7GbFacrM1lC4ATI2J1jmVpWJ09nu0/GsyZWpmQ6SBjVpSsNZlbSEaV2W7o6u7zbP9RYNK4VjraWli9wUHGrChZazKXABdKmksy87+/emdE/K7eBWsknT19HDBrYtnFaHqS2HdKO09scmoZs6JkDTLXpK+XDrHPHf8j6Orud01mlJgzZbz7ZMwKlDXILMy1FA2sd2AbW3oHnLdslJgzpZ2bV6wruxhmTSPrjP9H8i5Io6okx/Rs/9FhzpR2nty0lYFtg7R6QqZZ7nYaZCS9BfiviOhP3++UJ2Pu3Pa8ZR5dNirsO2U8gwFrt/RuH9JsZvkZriZzDbAv8BQ7+mSG4j6ZYVRm+7tPZnSYs31C5lYHGbMC7DTIRMSYod7brnHestGlMldmzYatsH/JhTFrAg4eOdtek3HH/6gwZ3Jl1r+HMZsVYVcSZO4HvJyhE2R+vs7lahhdace/m8tGh8njWxk/tsWz/s0KkjVB5juAy4ABYC1JP0xFAA4yO9HZ3cek9lanlh8lJDFnajtrNjnImBUha03mk8C/Ah+PiG05lqfhdDlv2agzZ0o7j3X2sHpDMU1mEuw7uR1JhdzPbDTJGmRmA19zgNl1nc5bNurMnzqBby9/jJdccFNh9zzrNQfxgdcsKux+ZqNF1iBzHXAM8FCOZWlIXT197DOpvexiWJWzXnsQL3zONOIZrb75+fJNK7hz1YZC7mU22mQNMjcAn5F0KEMnyMw0GVPS8SRr07SQ1IwuqNmvdP8JQA9wWiX55s7OlTQd+DbJcgQrgbdGRFfVNfcH7gXOjYgLMz5v3XR19/O82ZOLvq0NY98p7bz1RfsVdr+f37+W+5/YXNj9zEaTrEHmq+nrPw2xL9NkTEktwEXAa4FVwO2SlkXEvVWHLQEWpT/HABcDx4xw7tnAjRFxgaSz08//WHXNLwA/yvicdZc0l3n4cjNbOLODG+590qlsrCll/Rc/CRg7xKqYu7Iy5tHAioh4KCL6gKuApTXHLAWujMStwFRJc0Y4dylwRfr+CuCkysUknUTSxHdPxjLW1db+bTzdv82rYja5hTM7GBgMVnV5bo41nxGDTFqL2MCeL1o2D3is6vOqdFuWY4Y7d3ZErAFIX/dJy91BUqM5b7hCSTpd0nJJy9euXbtLDzQS5y0zSIIMwMPruksuiVnxRgwy6YiyR4A9/aYcavxmbc/rzo7Jcm6t84AvRMSW4Q6KiEsjYnFELJ41a9YIl9w1nc5bZuwIMg85yFgTyton8yngAkl/ExG7uxjHKqC6t3U+sDrjMW3DnPukpDkRsSZtWnsq3X4M8N8kfRaYCgxK2hoR/76b5d9lXc5bZiR//5PbW3l43bC/75g1pKxB5kMkC5c9LmkV8IxfySLi8AzXuB1YJGkh8DhwCvD2mmOWAe+TdBVJkNiYBo+1w5y7DDgVuCB9vTYt08srF5V0LrClyAADO1LKeMGy5iaJhbMmsnJdT9lFMSvcri6/vNsiYkDS+4DrSUajXRYR90g6I91/Ccl8nBOAFSRDmN813LnppS8Arpb0HuBR4OQ9LWu9OM2/VSycMYHbV3aNfKBZg8m6MuawnedZRcR1JIGketslVe8DODPruen29cBxI9z33N0o7h7r7O5DginjXZNpdgtnTuT7f1jN1v5ttI/18kvWPDxoP0ddPX1MGT/WcyOMhbOSzv9H1rvJzJpLpm8/SW2SzpP0gKStkrZV/+RdyL1VZ3cf091UZsDCGZVhzO78t+aS9VfsT5F0qv8rMAh8mGQG/nrgvfkUbe/X1dPniZgGwIKZEwAPY7bmkzXIvBU4IyK+CmwDro2I9wOfIEn1YkPo7O53ShkDYFL7WGZNGsfDax1krLlkDTKzSZJMAmwhmXcC8GPgdfUuVKPocpp/q7JwZgcr1zvIWHPJGmQeBeam71cAr0/fvxhwQqYhRASdXrDMqiyc0eHUMtZ0sgaZ77FjmPAXgfMkPQxcDnwth3Lt9Xr6ttE3MOg+Gdtu4awO1m3pY9PW/pEPNmsQWefJnFP1/pp01v9LgAci4gd5FW5vtj2ljJvLLFXJYbZyXTeHz586wtFmjSHrjP9nSNPw31rnsjSUru7kt1XXZKzigKpszA4y1iwyBxlJS0hm4x8AvD4iHpP0P4CHI+LGvAq4t+rcnhzTo8sssd/0CUjw6xXr2XdyMUtyjxvbwuHzpjBmzFCJzM3ylynISHoHcAlJ/8txQOWbswX4COAgU8N5y6xW+9gWFs7s4NvLH+Pbyx8b+YQ6+fqpiznu4NmF3c+sWtaazEeAv42Iq9LaS8WtwCfrX6y9nxcss6F88z3HsLKgEWbbIjjtG7fzu0e7HGSsNFmDzCLgN0Ns3wJMrl9xGkdXTx9jBJPb3VxmO8ydOp65U8cXdr9F+0zk7sc3FXY/s1pZhzCvBg4aYvsrgD/XrziNo7O7j6kT2twWbqV6/rwp3P34RpIE52bFyxpkLgW+JOml6ef9JJ0KfBa4OJeS7eW6evqcUsZK9/y5k1nf3ceTm3rLLoo1qazzZD4raQpwA9AO/AzoBS6MiItyLN9eq7Pbs/2tfIfNnwLAXY9vZN8pxYxoM6uWeaGTiPgoMBM4GjgWmBURH8+rYHu7ru5+jyyz0h08ZzIS3P34xrKLYk1qlyZjRkQPsDynsjSUzp4+jtzfE+6sXBPaWjlw1kTuWe0gY+XYaZCRtCzrRSLixPoUpzFEBBu8loyNEofNm8Jv/ry+7GJYkxquJuN/lbtpS+8A/dvCectsVDh07mS+9/vHWbu5l1mTxpVdHGsyOw0yEfGuIgvSSJy3zEaT589LOv/vXr2Rv3zePiWXxppN5o5/y855y2w0OXRuMl/6Hnf+WwkcZHLgvGU2mkxqH8vCmR2e+W+lcJDJgfOW2Whz6NzJ3OWajJXAQSYHlQXLpromY6PEYfOm8PiGp7fXss2KsluLltnwOrv7aBkjJrf7j9dGh0rn/znfvYsZE4v55Wfu1PG891UHIjl/XzPzt2AOkrxlbf7PZaPGEftN5XmzJ7H8kc5C7o1ju24AAA5oSURBVNc7MMjmrQO89pDZHDR7UiH3tNHJQSYHSd4yjyyz0WPiuFauP+sVhd1vVVcPL/vMz7hlxToHmSbnPpkcOG+ZNbv50ybwnBkTuGWF53Q3OweZHHT1OAOz2UsOnMltD61nYNtg2UWxEhUaZCQdL+l+SSsknT3Efkn6Urr/TklHjXSupOmSbpD0YPo6Ld3+Wkl3SLorfX11MU+Z9sk4yFiTe+lzZ7C5d8BDp5tcYUFGUgtwEbAEOAR4m6RDag5bQrLU8yLgdNIF0UY492zgxohYBNyYfgZYB7wpIg4DTgX+b06P9gyDg0FXT7/zllnTe/EBMwD4tZNzNrUiazJHAysi4qGI6AOuApbWHLMUuDIStwJTJc0Z4dylwBXp+yuAkwAi4vcRsTrdfg/QLin37ICbtw6wbTBck7GmN2PiOA6eM5lbVqwruyhWoiKDzDzgsarPq9JtWY4Z7tzZEbEGIH0dKgPgXwG/j4jc16Ct5C3z0stm8NIDZ7D8kS629m8ruyhWkiKDzFCTRiLjMVnOHfqm0qHAZ4C/28n+0yUtl7R87dq1WS45LKeUMdvhpc+dSd/AIHc80lV2UawkRQaZVcB+VZ/nA6szHjPcuU+mTWqkr09VDpI0H/ge8M6I+PNQhYqISyNicUQsnjVr1i4/VC0nxzTb4eiF02kdIzeZNbEig8ztwCJJCyW1AacAtatvLgPemY4yOxbYmDaBDXfuMpKOfdLXawEkTQV+CJwTEbfk+WDVdqT5d5Ax6xjXygv2m8ot7vxvWoXN+I+IAUnvA64HWoDLIuIeSWek+y8BrgNOAFYAPcC7hjs3vfQFwNWS3gM8Cpycbn8f8Fzg45I+nm57XURsr+nkYXtNxkHGDICXPHcmX77pQf7ywp8Xds/XHLwPH31D7eBVK0OhaWUi4jqSQFK97ZKq9wGcmfXcdPt64Lghtn8a+PQeFnmXdfb00dYyho62lqJvbTYqvXXxfB7vepr+giZlPrRuC9+4ZSXvfdVz/cveKODcZXXW1d3HtI6xTo5plpo/bQL/+tYjCrvf3Y9v5I1fvpkf3f0Ebz9m/8Lua0NzWpk66+px3jKzMh06dzIHzurg2j88XnZRDAeZuuvqdt4yszJJ4sQj5vHblZ08sXFr2cVpeg4yddbpvGVmpTvxBXOJgB/cWTtLwormIFNnXd19zltmVrKFMzs4fP4Urv2Dg0zZHGTqaNtgsOHpftdkzEaBE4+Yy12Pb+Thdd1lF6WpeXRZHW18up8ImO68ZWale+Phczn/uvv47u9WccYrDyzknhJMaPPXajX/adRRpydimo0a+05p55iF0/nyTSv48k0rCrvvWa85iA+8ZlFh9xvtHGTqqKvHecvMRpPz33wYN973ZGH3u+HeJ/nazQ/x7pctYFK7WzTAQaaunIHZbHQ5cNZEDpw1sbD7HXvADE7891v4z9se5e8KaqIb7dzxX0fOW2bW3A6fP5WXHDiDr9/8ML0DXkMHHGTqansGZjeXmTWtv3/VgTy1uZfv/94ZB8BBpq429PTTPnYM450c06xpvey5Mzl07mS++suHGBzMtLZiQ3OfTB11eiKmWdOTxBmvPJD/+a3f88kf3Mv8aeMLue+k9lZOOnIe41pH1y+5DjJ1lGRgdpAxa3ZLnr8vB82eyOW/XlnofR9a2805Jxxc6D1H4iBTR509To5pZtDaMobr3v9ynu4vrvP/Uz+4l6/d/DAnHTmPg+dMLuy+I3GQqaOu7j72mzah7GKY2SjQ2jKGSS3FdXufs+RgfnrfU/zT9+7iO2e8hDFjRseaVu74r6NOp/k3s5JM62jjoycczO8f3cC3bn+07OJs55pMnfRvG2TT1gGmOm+ZmZXkLUfN45o7VvGZH/2JtZt7EdlrMwfNnsiSw+bUvUwOMnWyoacf8Gx/MyuPJD795udzyqW38m8/fXCXzn3j4XMcZEYz5y0zs9HgwFkT+e0/HUeMkik6DjJ14rxlZjZaSEKjo9/fHf/1sj1vmWsyZmbbOcjUSZf7ZMzMnsVBpk4qfTIeXWZmtoODTJ10dvfR0dZC+9jRlTfIzKxMDjJ14rxlZmbP5iBTJ85bZmb2bA4yddLV3eeRZWZmNRxk6qSzp49p7vQ3M3sGB5k66erud5+MmVmNQoOMpOMl3S9phaSzh9gvSV9K998p6aiRzpU0XdINkh5MX6dV7TsnPf5+Sa/P67l6B7axpXfAq2KamdUoLMhIagEuApYAhwBvk3RIzWFLgEXpz+nAxRnOPRu4MSIWATemn0n3nwIcChwPfCW9Tt1VkmO6JmNm9kxF1mSOBlZExEMR0QdcBSytOWYpcGUkbgWmSpozwrlLgSvS91cAJ1VtvyoieiPiYWBFep26c94yM7OhFRlk5gGPVX1elW7Lcsxw586OiDUA6es+u3A/JJ0uabmk5WvXrt2lB6oY1zqGNxw2h+fM8KqYZmbVigwyQ+UErU1GvbNjspy7O/cjIi6NiMURsXjWrFkjXHJoB8yayEXvOIpD507ZrfPNzBpVkUFmFbBf1ef5wOqMxwx37pNpkxrp61O7cD8zM8tRkUHmdmCRpIWS2kg65ZfVHLMMeGc6yuxYYGPaBDbcucuAU9P3pwLXVm0/RdI4SQtJBhP8Nq+HMzOzZyts0bKIGJD0PuB6oAW4LCLukXRGuv8S4DrgBJJO+h7gXcOdm176AuBqSe8BHgVOTs+5R9LVwL3AAHBmRGwr5mnNzAxAMVrW6BwFFi9eHMuXLy+7GGZmexVJd0TE4qH2eca/mZnlxkHGzMxy4yBjZma5cZAxM7PcuOO/iqS1wCN7cImZwLo6FWdv0YzPDM353H7m5rGrz/2ciBhyNruDTB1JWr6zERaNqhmfGZrzuf3MzaOez+3mMjMzy42DjJmZ5cZBpr4uLbsAJWjGZ4bmfG4/c/Oo23O7T8bMzHLjmoyZmeXGQcbMzHLjIFMHko6XdL+kFZLOLrs8eZC0n6SfSbpP0j2SPpBuny7pBkkPpq/Tyi5rHiS1SPq9pB+knxv6uSVNlXSNpD+lf+cvbvRnBpB0Vvrv+25J35LU3ojPLekySU9Jurtq206fU9I56ffb/ZJevyv3cpDZQ5JagIuAJcAhwNskHVJuqXIxAHwwIg4GjgXOTJ/zbODGiFgE3Jh+bkQfAO6r+tzoz/1F4McR8RfAESTP3tDPLGke8H5gcUQ8n2RZkVNozOe+HDi+ZtuQz5n+Pz8FODQ95yvp914mDjJ77mhgRUQ8FBF9wFXA0pLLVHcRsSYifpe+30zypTOP5FmvSA+7AjipnBLmR9J84A3A16o2N+xzS5oMvAL4OkBE9EXEBhr4mau0AuMltQITSFbTbbjnjohfAp01m3f2nEuBqyKiNyIeJlnv6+is93KQ2XPzgMeqPq9KtzUsSQuAI4HbgNnp6qWkr/uUV7Lc/BvwEWCwalsjP/cBwFrgG2kT4dckddDYz0xEPA5cSLL44RqSlXl/QoM/d5WdPecefcc5yOw5DbGtYceFS5oIfAf4h4jYVHZ58ibpjcBTEXFH2WUpUCtwFHBxRBwJdNMYTUTDSvsglgILgblAh6S/KbdUo8Iefcc5yOy5VcB+VZ/nk1SxG46ksSQB5j8i4rvp5iclzUn3zwGeKqt8OXkpcKKklSRNoa+W9E0a+7lXAasi4rb08zUkQaeRnxngNcDDEbE2IvqB7wIvofGfu2Jnz7lH33EOMnvudmCRpIWS2kg6yJaVXKa6kySSNvr7IuLzVbuWAaem708Fri26bHmKiHMiYn5ELCD5u70pIv6GBn7uiHgCeEzS89JNxwH30sDPnHoUOFbShPTf+3EkfY+N/twVO3vOZcApksZJWggsAn6b9aKe8V8Hkk4gabdvAS6LiPNLLlLdSXoZ8CvgLnb0TfwTSb/M1cD+JP9JT46I2g7FhiDpVcCHIuKNkmbQwM8t6QUkAx3agIeAd5H8Utqwzwwg6Tzgr0lGU/4e+B/ARBrsuSV9C3gVSUr/J4FPAN9nJ88p6aPAu0n+XP4hIn6U+V4OMmZmlhc3l5mZWW4cZMzMLDcOMmZmlhsHGTMzy42DjJmZ5cZBxqxBSVogKSQtLrss1rwcZMzMLDcOMmZmlhsHGbOcKPERSX+W9LSkuyoJF6uast4u6WZJW9MFwl5Xc41XSLot3f+kpC+k6Yuq7/HBdKGpXkmrJP1LTVGeky5C1SPpXkmvLeDxzQAHGbM8fRp4D3AmyYJ2/wJ8VdIbqo75LPAl4AXADcC16eJZlUW0fkSS3uTI9FpvS69T8c/Ax9NthwIn88y07ADnp/c4giTX3lVpNm2z3DmtjFkO0vVX1gGvi4hfVW3/N+Ag4L3Aw8DHKrnuJI0B/gRcHREfk3Q+SR6tgyJiMD3mNOCrwDSSXxLXkeSSumSIMixI73FGRHw13TaPJKvuyyPi5vo/udkztZZdALMGdQjQDvxYUvVvcmOBlVWff1N5ExGDkm5LzwU4GPhNJcCkbiZJWvnc9PrjSJbKHc6dVe8rKdobdeEtG2UcZMzyUWmKfhNJRttq/Qy9EFQtsfPFoSLjNSr3S06KiCSLvZvKrRj+h2aWj3uBXuA5EbGi5ueRquOOrbxJ1zA5mmQNk8o1Xpw2o1W8DOgD/lx1j+NyfA6zPeKajFkOImKzpAuBC9Pg8UuSdUmOJVmP5yfpoX8v6QGSdXreCzwHuDjd9xXgH4CvSPoicABwAfDvEdEDkG7/F0m96T1mAC+MiMo1zErlIGOWn4+TLAj1IZLAsQn4A8mIsoqzgf9FsrzxI8CbI2IVQEQ8LmkJ8Ln0vA3Af5IsFldxDtCV3mt+er8r83sks13j0WVmJaga+fWiiFhebmnM8uM+GTMzy42DjJmZ5cbNZWZmlhvXZMzMLDcOMmZmlhsHGTMzy42DjJmZ5cZBxszMcvP/Aci+EBNOwlrpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CUSTOM LEARNING SCHEUDLE\n",
    "LR_START = 1e-6\n",
    "LR_MAX = 1e-3\n",
    "LR_RAMPUP_EPOCHS = 5\n",
    "LR_SUSTAIN_EPOCHS = 0\n",
    "LR_STEP_DECAY = 0.75\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = LR_MAX * LR_STEP_DECAY**((epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS)//10)\n",
    "    return lr\n",
    "    \n",
    "lr2 = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "\n",
    "rng = [i for i in range(100)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y); \n",
    "plt.xlabel('epoch',size=14); plt.ylabel('learning rate',size=14)\n",
    "plt.title('Training Schedule',size=16); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.019566,
     "end_time": "2020-11-15T10:04:30.986688",
     "exception": false,
     "start_time": "2020-11-15T10:04:30.967122",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:31.048843Z",
     "iopub.status.busy": "2020-11-15T10:04:31.046765Z",
     "iopub.status.idle": "2020-11-15T10:04:31.049675Z",
     "shell.execute_reply": "2020-11-15T10:04:31.050242Z"
    },
    "papermill": {
     "duration": 0.043019,
     "end_time": "2020-11-15T10:04:31.050390",
     "exception": false,
     "start_time": "2020-11-15T10:04:31.007371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(train, test, train_targets,features, FOLDS, SEED ,MODEL):\n",
    "    \n",
    "    \n",
    "    seed_everything(SEED) \n",
    "    oof_pred = np.zeros((train.shape[0], 206))\n",
    "    test_pred = np.zeros((test.shape[0], 206)) \n",
    "    \n",
    "    for fold, (trn_ind, val_ind) in enumerate(MultilabelStratifiedKFold(n_splits = FOLDS, random_state = SEED, \n",
    "                                                                        shuffle = True).split(train, train_targets)):\n",
    "        \n",
    "        \n",
    "        print('#'*25)\n",
    "        print('### FOLD %i' % (fold+1))\n",
    "        print(f'###Training model {MODEL} with seed {SEED}')\n",
    "        print('### train on %i data validate on %i data' % (len(trn_ind),len(val_ind)))\n",
    "        print('#'*25)\n",
    "    \n",
    "        x_train, x_test = train[features].values[trn_ind], train[features].values[val_ind]\n",
    "        y_train, y_test = train_targets.values[trn_ind], train_targets.values[val_ind]\n",
    "        \n",
    "        K.clear_session()\n",
    "        if MODEL == '4l':\n",
    "            model = build_model_4l(len(features))\n",
    "        elif MODEL == '3l':\n",
    "            model = build_model_3l(len(features))\n",
    "        \n",
    "        #model = build_model(x_train,y_train, initial_bias)\n",
    "        \n",
    "        checkpoint_path = f'Best_weights_Fold:{fold}.hdf5'\n",
    "        \n",
    "        #reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=PATIENCE, verbose=VERBOSE, epsilon=1e-4, mode='min')\n",
    "        \n",
    "        #early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
    "        #                                                    mode = 'min',\n",
    "        #                                                    patience = PATIENCE,\n",
    "        #                                                    restore_best_weights = True,\n",
    "        #                                                    verbose = VERBOSE)\n",
    "        \n",
    "        model_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 2, save_best_only = True,\n",
    "                                         save_weights_only = True, mode = 'min')   \n",
    "    \n",
    "        \n",
    "        history = model.fit( x_train, y_train,\n",
    "                    validation_data= (x_test, y_test),\n",
    "                    verbose= 2, \n",
    "                    epochs= EPOCHS, \n",
    "                    batch_size= 128,\n",
    "                    callbacks = [ lr2, model_checkpt ])\n",
    "        \n",
    "        model.load_weights(checkpoint_path)\n",
    "        oof_pred[val_ind] = model.predict(x_test, batch_size = 128)\n",
    "        test_pred += model.predict(test[features].values) / FOLDS\n",
    "        \n",
    "    oof_score = mean_log_loss(train_targets.values, oof_pred)\n",
    "    print(f'FOLD mean log loss score is {oof_score}')\n",
    "    \n",
    "    return test_pred, oof_pred, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:31.105253Z",
     "iopub.status.busy": "2020-11-15T10:04:31.103157Z",
     "iopub.status.idle": "2020-11-15T10:04:31.106058Z",
     "shell.execute_reply": "2020-11-15T10:04:31.106578Z"
    },
    "papermill": {
     "duration": 0.035326,
     "end_time": "2020-11-15T10:04:31.106702",
     "exception": false,
     "start_time": "2020-11-15T10:04:31.071376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to train our model with multiple seeds and average the predictions\n",
    "def run_multiple_seeds(train, test, train_targets, FOLDS, SEEDS = [12], MODEL = '4l'):\n",
    "    \n",
    "    test1_pred = []\n",
    "    oof1_pred = []\n",
    "    \n",
    "    for SEED in SEEDS:\n",
    "        print(f'Training model {MODEL} with seed {SEED}')\n",
    "        train_, test_ = fe_pca(train, test, n_components_g = 70, n_components_c = 10, SEED = SEED)\n",
    "        train_, test_, features = scaling(train_, test_)\n",
    "        #print(f'Training with {len(features)} features')\n",
    "        test_pred_, oof_pred_ = train_model(train_, test_, train_targets,features, FOLDS, SEED = SEED, MODEL = MODEL)\n",
    "        test1_pred.append(test_pred_)\n",
    "        oof1_pred.append(oof_pred_)\n",
    "        print('-'*50)\n",
    "        print('\\n')\n",
    "        \n",
    "    test1_pred = np.average(test1_pred, axis = 0)\n",
    "    oof1_pred = np.average(oof1_pred, axis = 0)\n",
    "        \n",
    "    seed_log_loss = mean_log_loss(train_targets.values, oof1_pred)\n",
    "    print(f'Our out of folds log loss for our seed blend model is {seed_log_loss}')\n",
    "    \n",
    "    return test1_pred, oof1_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021126,
     "end_time": "2020-11-15T10:04:31.148343",
     "exception": false,
     "start_time": "2020-11-15T10:04:31.127217",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# initialyse the initial weight and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:04:31.216257Z",
     "iopub.status.busy": "2020-11-15T10:04:31.215422Z",
     "iopub.status.idle": "2020-11-15T10:12:47.746353Z",
     "shell.execute_reply": "2020-11-15T10:12:47.745683Z"
    },
    "papermill": {
     "duration": 496.577401,
     "end_time": "2020-11-15T10:12:47.746486",
     "exception": false,
     "start_time": "2020-11-15T10:04:31.169085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets Tp:\n",
      "    Total: 206\n",
      "    Positive: 33 (16.02% of total)\n",
      "\n",
      "Weights initial biais: [-1.65678403]\n",
      "#########################\n",
      "### FOLD 1\n",
      "###Training model 4l with seed 34\n",
      "### train on 17558 data validate on 4390 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20788, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 2s - loss: 0.3712 - auc: 0.4972 - val_loss: 0.2079 - val_auc: 0.5355\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20788 to 0.19805, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.2813 - auc: 0.5527 - val_loss: 0.1981 - val_auc: 0.6225\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19805 to 0.10628, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.1774 - auc: 0.5742 - val_loss: 0.1063 - val_auc: 0.6507\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10628 to 0.03454, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0628 - auc: 0.6008 - val_loss: 0.0345 - val_auc: 0.7409\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03454 to 0.02652, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0311 - auc: 0.6866 - val_loss: 0.0265 - val_auc: 0.7581\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02652 to 0.02437, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0266 - auc: 0.7458 - val_loss: 0.0244 - val_auc: 0.7870\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02437 to 0.02366, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0250 - auc: 0.7661 - val_loss: 0.0237 - val_auc: 0.7874\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02366 to 0.02328, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0244 - auc: 0.7760 - val_loss: 0.0233 - val_auc: 0.7937\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02328 to 0.02306, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0241 - auc: 0.7837 - val_loss: 0.0231 - val_auc: 0.8043\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02306 to 0.02285, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0238 - auc: 0.7924 - val_loss: 0.0228 - val_auc: 0.8004\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02285 to 0.02260, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0235 - auc: 0.7998 - val_loss: 0.0226 - val_auc: 0.8117\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02260 to 0.02258, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 2s - loss: 0.0233 - auc: 0.8041 - val_loss: 0.0226 - val_auc: 0.8190\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02258 to 0.02242, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0232 - auc: 0.8090 - val_loss: 0.0224 - val_auc: 0.8248\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02242 to 0.02229, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0229 - auc: 0.8147 - val_loss: 0.0223 - val_auc: 0.8235\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02229 to 0.02213, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 2s - loss: 0.0227 - auc: 0.8199 - val_loss: 0.0221 - val_auc: 0.8299\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.02213\n",
      "138/138 - 1s - loss: 0.0226 - auc: 0.8238 - val_loss: 0.0222 - val_auc: 0.8316\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02213 to 0.02209, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0225 - auc: 0.8285 - val_loss: 0.0221 - val_auc: 0.8261\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02209 to 0.02199, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0223 - auc: 0.8299 - val_loss: 0.0220 - val_auc: 0.8238\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02199\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8318 - val_loss: 0.0220 - val_auc: 0.8423\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02199 to 0.02191, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0222 - auc: 0.8368 - val_loss: 0.0219 - val_auc: 0.8325\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02191 to 0.02190, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8402 - val_loss: 0.0219 - val_auc: 0.8351\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02190 to 0.02175, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8412 - val_loss: 0.0217 - val_auc: 0.8362\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02175\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8435 - val_loss: 0.0218 - val_auc: 0.8304\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02175\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8463 - val_loss: 0.0218 - val_auc: 0.8395\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02175 to 0.02157, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0217 - auc: 0.8491 - val_loss: 0.0216 - val_auc: 0.8409\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02157\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8540 - val_loss: 0.0216 - val_auc: 0.8415\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02157\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8571 - val_loss: 0.0216 - val_auc: 0.8443\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02157 to 0.02150, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8622 - val_loss: 0.0215 - val_auc: 0.8414\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02150\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8635 - val_loss: 0.0215 - val_auc: 0.8393\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02150\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8632 - val_loss: 0.0215 - val_auc: 0.8437\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.02150 to 0.02140, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8664 - val_loss: 0.0214 - val_auc: 0.8397\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02140\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8660 - val_loss: 0.0215 - val_auc: 0.8425\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.02140 to 0.02135, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8699 - val_loss: 0.0213 - val_auc: 0.8442\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02135\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8721 - val_loss: 0.0215 - val_auc: 0.8409\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02135\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8741 - val_loss: 0.0214 - val_auc: 0.8377\n",
      "#########################\n",
      "### FOLD 2\n",
      "###Training model 4l with seed 34\n",
      "### train on 17559 data validate on 4389 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20413, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 2s - loss: 0.3710 - auc: 0.5088 - val_loss: 0.2041 - val_auc: 0.5276\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20413 to 0.20363, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.2830 - auc: 0.5520 - val_loss: 0.2036 - val_auc: 0.6530\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.20363 to 0.10577, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.1778 - auc: 0.5735 - val_loss: 0.1058 - val_auc: 0.6777\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10577 to 0.03391, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 2s - loss: 0.0617 - auc: 0.6041 - val_loss: 0.0339 - val_auc: 0.7441\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03391 to 0.02644, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0307 - auc: 0.6881 - val_loss: 0.0264 - val_auc: 0.7606\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02644 to 0.02444, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0264 - auc: 0.7420 - val_loss: 0.0244 - val_auc: 0.7861\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02444 to 0.02361, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0252 - auc: 0.7646 - val_loss: 0.0236 - val_auc: 0.7859\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02361 to 0.02343, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0244 - auc: 0.7805 - val_loss: 0.0234 - val_auc: 0.7988\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02343 to 0.02309, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0242 - auc: 0.7864 - val_loss: 0.0231 - val_auc: 0.7999\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02309 to 0.02288, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0237 - auc: 0.7932 - val_loss: 0.0229 - val_auc: 0.8000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02288 to 0.02283, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0234 - auc: 0.8006 - val_loss: 0.0228 - val_auc: 0.8193\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02283 to 0.02276, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0233 - auc: 0.8065 - val_loss: 0.0228 - val_auc: 0.8091\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02276 to 0.02256, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0230 - auc: 0.8110 - val_loss: 0.0226 - val_auc: 0.8252\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02256 to 0.02235, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0228 - auc: 0.8182 - val_loss: 0.0224 - val_auc: 0.8202\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02235 to 0.02233, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8218 - val_loss: 0.0223 - val_auc: 0.8231\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02233 to 0.02229, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0225 - auc: 0.8261 - val_loss: 0.0223 - val_auc: 0.8376\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02229 to 0.02218, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0226 - auc: 0.8272 - val_loss: 0.0222 - val_auc: 0.8180\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02218 to 0.02218, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8334 - val_loss: 0.0222 - val_auc: 0.8218\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02218 to 0.02211, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0223 - auc: 0.8326 - val_loss: 0.0221 - val_auc: 0.8274\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02211 to 0.02199, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8376 - val_loss: 0.0220 - val_auc: 0.8262\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02199\n",
      "138/138 - 2s - loss: 0.0222 - auc: 0.8396 - val_loss: 0.0220 - val_auc: 0.8316\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02199 to 0.02187, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8439 - val_loss: 0.0219 - val_auc: 0.8302\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02187\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8474 - val_loss: 0.0220 - val_auc: 0.8326\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02187 to 0.02186, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 2s - loss: 0.0219 - auc: 0.8471 - val_loss: 0.0219 - val_auc: 0.8370\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02186 to 0.02173, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8499 - val_loss: 0.0217 - val_auc: 0.8357\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02173\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8575 - val_loss: 0.0218 - val_auc: 0.8357\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02173 to 0.02162, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8595 - val_loss: 0.0216 - val_auc: 0.8382\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02162 to 0.02161, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8631 - val_loss: 0.0216 - val_auc: 0.8356\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8639 - val_loss: 0.0217 - val_auc: 0.8357\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8641 - val_loss: 0.0217 - val_auc: 0.8387\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8657 - val_loss: 0.0217 - val_auc: 0.8367\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.02161 to 0.02161, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8685 - val_loss: 0.0216 - val_auc: 0.8384\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8713 - val_loss: 0.0217 - val_auc: 0.8361\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8730 - val_loss: 0.0217 - val_auc: 0.8376\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8726 - val_loss: 0.0216 - val_auc: 0.8421\n",
      "#########################\n",
      "### FOLD 3\n",
      "###Training model 4l with seed 34\n",
      "### train on 17559 data validate on 4389 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20900, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 2s - loss: 0.3714 - auc: 0.4929 - val_loss: 0.2090 - val_auc: 0.4939\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20900 to 0.19861, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.2825 - auc: 0.5526 - val_loss: 0.1986 - val_auc: 0.6011\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19861 to 0.10391, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.1759 - auc: 0.5814 - val_loss: 0.1039 - val_auc: 0.6846\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10391 to 0.03398, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0613 - auc: 0.6010 - val_loss: 0.0340 - val_auc: 0.7492\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03398 to 0.02607, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0308 - auc: 0.6892 - val_loss: 0.0261 - val_auc: 0.7607\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02607 to 0.02428, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0267 - auc: 0.7390 - val_loss: 0.0243 - val_auc: 0.7922\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02428 to 0.02362, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0253 - auc: 0.7628 - val_loss: 0.0236 - val_auc: 0.7869\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02362 to 0.02316, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0246 - auc: 0.7757 - val_loss: 0.0232 - val_auc: 0.7881\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02316 to 0.02277, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0241 - auc: 0.7869 - val_loss: 0.0228 - val_auc: 0.8024\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02277 to 0.02266, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0237 - auc: 0.7953 - val_loss: 0.0227 - val_auc: 0.7980\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02266 to 0.02248, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0236 - auc: 0.8004 - val_loss: 0.0225 - val_auc: 0.8083\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02248 to 0.02228, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 2s - loss: 0.0233 - auc: 0.8054 - val_loss: 0.0223 - val_auc: 0.8158\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02228 to 0.02226, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0231 - auc: 0.8109 - val_loss: 0.0223 - val_auc: 0.8292\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02226 to 0.02203, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0230 - auc: 0.8137 - val_loss: 0.0220 - val_auc: 0.8207\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02203\n",
      "138/138 - 1s - loss: 0.0230 - auc: 0.8181 - val_loss: 0.0220 - val_auc: 0.8295\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02203 to 0.02185, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0226 - auc: 0.8260 - val_loss: 0.0219 - val_auc: 0.8300\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02185 to 0.02181, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8263 - val_loss: 0.0218 - val_auc: 0.8316\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02181\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8318 - val_loss: 0.0219 - val_auc: 0.8424\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02181 to 0.02178, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8347 - val_loss: 0.0218 - val_auc: 0.8325\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02178 to 0.02177, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0222 - auc: 0.8375 - val_loss: 0.0218 - val_auc: 0.8357\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02177 to 0.02161, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8402 - val_loss: 0.0216 - val_auc: 0.8333\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8433 - val_loss: 0.0216 - val_auc: 0.8384\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8458 - val_loss: 0.0217 - val_auc: 0.8378\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02161 to 0.02159, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8479 - val_loss: 0.0216 - val_auc: 0.8391\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02159 to 0.02148, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0218 - auc: 0.8510 - val_loss: 0.0215 - val_auc: 0.8409\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02148 to 0.02142, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0218 - auc: 0.8541 - val_loss: 0.0214 - val_auc: 0.8368\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02142\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8575 - val_loss: 0.0214 - val_auc: 0.8420\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02142\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8582 - val_loss: 0.0215 - val_auc: 0.8395\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.02142 to 0.02131, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8625 - val_loss: 0.0213 - val_auc: 0.8443\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02131\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8628 - val_loss: 0.0215 - val_auc: 0.8430\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02131\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8677 - val_loss: 0.0214 - val_auc: 0.8430\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.02131 to 0.02127, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8700 - val_loss: 0.0213 - val_auc: 0.8444\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02127\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8711 - val_loss: 0.0213 - val_auc: 0.8395\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02127\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8721 - val_loss: 0.0213 - val_auc: 0.8421\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02127\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8756 - val_loss: 0.0215 - val_auc: 0.8448\n",
      "#########################\n",
      "### FOLD 4\n",
      "###Training model 4l with seed 34\n",
      "### train on 17558 data validate on 4390 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20745, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 2s - loss: 0.3720 - auc: 0.4890 - val_loss: 0.2074 - val_auc: 0.5305\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20745 to 0.19729, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.2814 - auc: 0.5527 - val_loss: 0.1973 - val_auc: 0.6170\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19729 to 0.10351, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.1773 - auc: 0.5771 - val_loss: 0.1035 - val_auc: 0.6720\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10351 to 0.03398, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0620 - auc: 0.6006 - val_loss: 0.0340 - val_auc: 0.7500\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03398 to 0.02613, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0308 - auc: 0.6873 - val_loss: 0.0261 - val_auc: 0.7648\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02613 to 0.02417, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 2s - loss: 0.0265 - auc: 0.7417 - val_loss: 0.0242 - val_auc: 0.7886\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02417 to 0.02342, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0252 - auc: 0.7671 - val_loss: 0.0234 - val_auc: 0.7956\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02342 to 0.02293, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0245 - auc: 0.7766 - val_loss: 0.0229 - val_auc: 0.7964\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02293 to 0.02272, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0241 - auc: 0.7849 - val_loss: 0.0227 - val_auc: 0.8108\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02272 to 0.02251, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0239 - auc: 0.7914 - val_loss: 0.0225 - val_auc: 0.8045\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02251 to 0.02235, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0235 - auc: 0.8022 - val_loss: 0.0223 - val_auc: 0.8147\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02235 to 0.02233, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0233 - auc: 0.8030 - val_loss: 0.0223 - val_auc: 0.8156\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02233 to 0.02205, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0231 - auc: 0.8097 - val_loss: 0.0220 - val_auc: 0.8198\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02205 to 0.02202, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0230 - auc: 0.8132 - val_loss: 0.0220 - val_auc: 0.8253\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02202 to 0.02188, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0228 - auc: 0.8198 - val_loss: 0.0219 - val_auc: 0.8274\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02188 to 0.02185, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8247 - val_loss: 0.0219 - val_auc: 0.8287\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02185 to 0.02179, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0225 - auc: 0.8286 - val_loss: 0.0218 - val_auc: 0.8314\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02179 to 0.02171, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8290 - val_loss: 0.0217 - val_auc: 0.8374\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02171 to 0.02164, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8347 - val_loss: 0.0216 - val_auc: 0.8335\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0223 - auc: 0.8367 - val_loss: 0.0216 - val_auc: 0.8326\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8403 - val_loss: 0.0217 - val_auc: 0.8456\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02164 to 0.02161, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8400 - val_loss: 0.0216 - val_auc: 0.8327\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02161 to 0.02153, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8456 - val_loss: 0.0215 - val_auc: 0.8382\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02153 to 0.02146, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 2s - loss: 0.0219 - auc: 0.8479 - val_loss: 0.0215 - val_auc: 0.8382\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02146\n",
      "138/138 - 1s - loss: 0.0218 - auc: 0.8479 - val_loss: 0.0215 - val_auc: 0.8424\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02146 to 0.02143, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8560 - val_loss: 0.0214 - val_auc: 0.8415\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02143 to 0.02132, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8566 - val_loss: 0.0213 - val_auc: 0.8390\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02132 to 0.02128, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8600 - val_loss: 0.0213 - val_auc: 0.8429\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02128\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8624 - val_loss: 0.0213 - val_auc: 0.8427\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02128\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8633 - val_loss: 0.0213 - val_auc: 0.8457\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.02128 to 0.02128, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8667 - val_loss: 0.0213 - val_auc: 0.8448\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02128\n",
      "138/138 - 2s - loss: 0.0211 - auc: 0.8691 - val_loss: 0.0213 - val_auc: 0.8451\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.02128 to 0.02125, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8720 - val_loss: 0.0212 - val_auc: 0.8408\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02125 to 0.02122, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8725 - val_loss: 0.0212 - val_auc: 0.8462\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02122\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8748 - val_loss: 0.0213 - val_auc: 0.8467\n",
      "#########################\n",
      "### FOLD 5\n",
      "###Training model 4l with seed 34\n",
      "### train on 17558 data validate on 4390 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20734, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 2s - loss: 0.3713 - auc: 0.4985 - val_loss: 0.2073 - val_auc: 0.4900\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20734 to 0.19803, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 2s - loss: 0.2821 - auc: 0.5481 - val_loss: 0.1980 - val_auc: 0.6372\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19803 to 0.10573, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.1769 - auc: 0.5804 - val_loss: 0.1057 - val_auc: 0.6685\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10573 to 0.03443, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0621 - auc: 0.6008 - val_loss: 0.0344 - val_auc: 0.7455\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03443 to 0.02628, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 2s - loss: 0.0309 - auc: 0.6874 - val_loss: 0.0263 - val_auc: 0.7694\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02628 to 0.02424, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0265 - auc: 0.7439 - val_loss: 0.0242 - val_auc: 0.7873\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02424 to 0.02350, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0251 - auc: 0.7658 - val_loss: 0.0235 - val_auc: 0.7862\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02350 to 0.02312, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0243 - auc: 0.7774 - val_loss: 0.0231 - val_auc: 0.7964\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02312 to 0.02291, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0240 - auc: 0.7878 - val_loss: 0.0229 - val_auc: 0.8048\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02291 to 0.02267, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0236 - auc: 0.7953 - val_loss: 0.0227 - val_auc: 0.8115\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02267 to 0.02261, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 2s - loss: 0.0235 - auc: 0.7991 - val_loss: 0.0226 - val_auc: 0.8155\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02261 to 0.02236, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0233 - auc: 0.8057 - val_loss: 0.0224 - val_auc: 0.8175\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02236\n",
      "138/138 - 1s - loss: 0.0231 - auc: 0.8116 - val_loss: 0.0224 - val_auc: 0.8148\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02236 to 0.02216, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0231 - auc: 0.8141 - val_loss: 0.0222 - val_auc: 0.8248\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02216 to 0.02205, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8191 - val_loss: 0.0221 - val_auc: 0.8234\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02205 to 0.02195, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8248 - val_loss: 0.0220 - val_auc: 0.8256\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.02195\n",
      "138/138 - 1s - loss: 0.0226 - auc: 0.8285 - val_loss: 0.0220 - val_auc: 0.8373\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02195\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8308 - val_loss: 0.0220 - val_auc: 0.8394\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02195 to 0.02184, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0222 - auc: 0.8355 - val_loss: 0.0218 - val_auc: 0.8345\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02184\n",
      "138/138 - 1s - loss: 0.0222 - auc: 0.8385 - val_loss: 0.0219 - val_auc: 0.8384\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02184\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8400 - val_loss: 0.0219 - val_auc: 0.8415\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02184 to 0.02166, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8434 - val_loss: 0.0217 - val_auc: 0.8389\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02166\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8476 - val_loss: 0.0218 - val_auc: 0.8296\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02166 to 0.02161, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8479 - val_loss: 0.0216 - val_auc: 0.8366\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02161 to 0.02158, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0218 - auc: 0.8524 - val_loss: 0.0216 - val_auc: 0.8302\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02158 to 0.02149, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8554 - val_loss: 0.0215 - val_auc: 0.8339\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02149 to 0.02144, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8593 - val_loss: 0.0214 - val_auc: 0.8379\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02144 to 0.02143, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8651 - val_loss: 0.0214 - val_auc: 0.8403\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.02143 to 0.02141, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8647 - val_loss: 0.0214 - val_auc: 0.8363\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02141\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8654 - val_loss: 0.0215 - val_auc: 0.8424\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.02141 to 0.02140, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8686 - val_loss: 0.0214 - val_auc: 0.8407\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.02140 to 0.02132, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8710 - val_loss: 0.0213 - val_auc: 0.8450\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02132\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8718 - val_loss: 0.0214 - val_auc: 0.8356\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.02132 to 0.02131, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8754 - val_loss: 0.0213 - val_auc: 0.8442\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02131\n",
      "138/138 - 1s - loss: 0.0208 - auc: 0.8771 - val_loss: 0.0214 - val_auc: 0.8431\n",
      "FOLD mean log loss score is 0.016230728053897754\n",
      "#########################\n",
      "### FOLD 1\n",
      "###Training model 3l with seed 42\n",
      "### train on 17558 data validate on 4390 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21224, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.3710 - auc: 0.4865 - val_loss: 0.2122 - val_auc: 0.4897\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21224 to 0.19442, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.2771 - auc: 0.5489 - val_loss: 0.1944 - val_auc: 0.6323\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19442 to 0.10469, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.1784 - auc: 0.5826 - val_loss: 0.1047 - val_auc: 0.6710\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10469 to 0.03370, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0632 - auc: 0.6126 - val_loss: 0.0337 - val_auc: 0.7493\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03370 to 0.02621, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0308 - auc: 0.6943 - val_loss: 0.0262 - val_auc: 0.7687\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02621 to 0.02439, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0263 - auc: 0.7447 - val_loss: 0.0244 - val_auc: 0.7842\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02439 to 0.02368, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0250 - auc: 0.7678 - val_loss: 0.0237 - val_auc: 0.7880\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02368 to 0.02332, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0244 - auc: 0.7785 - val_loss: 0.0233 - val_auc: 0.8036\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02332 to 0.02314, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0239 - auc: 0.7879 - val_loss: 0.0231 - val_auc: 0.7991\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02314 to 0.02298, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0237 - auc: 0.7966 - val_loss: 0.0230 - val_auc: 0.8006\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02298 to 0.02260, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0235 - auc: 0.8019 - val_loss: 0.0226 - val_auc: 0.8084\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02260 to 0.02243, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0232 - auc: 0.8088 - val_loss: 0.0224 - val_auc: 0.8164\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.02243\n",
      "138/138 - 1s - loss: 0.0231 - auc: 0.8137 - val_loss: 0.0225 - val_auc: 0.8221\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02243 to 0.02225, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0228 - auc: 0.8215 - val_loss: 0.0223 - val_auc: 0.8263\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02225 to 0.02220, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 2s - loss: 0.0226 - auc: 0.8262 - val_loss: 0.0222 - val_auc: 0.8230\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02220 to 0.02201, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8325 - val_loss: 0.0220 - val_auc: 0.8253\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02201 to 0.02195, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0223 - auc: 0.8333 - val_loss: 0.0219 - val_auc: 0.8210\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02195 to 0.02186, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8418 - val_loss: 0.0219 - val_auc: 0.8232\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02186 to 0.02181, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8445 - val_loss: 0.0218 - val_auc: 0.8288\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02181\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8497 - val_loss: 0.0218 - val_auc: 0.8339\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02181 to 0.02173, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0218 - auc: 0.8517 - val_loss: 0.0217 - val_auc: 0.8382\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02173 to 0.02164, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8537 - val_loss: 0.0216 - val_auc: 0.8294\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8593 - val_loss: 0.0217 - val_auc: 0.8323\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8612 - val_loss: 0.0217 - val_auc: 0.8357\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8638 - val_loss: 0.0216 - val_auc: 0.8340\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8725 - val_loss: 0.0217 - val_auc: 0.8451\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02164 to 0.02139, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8749 - val_loss: 0.0214 - val_auc: 0.8411\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02139\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8786 - val_loss: 0.0214 - val_auc: 0.8342\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02139\n",
      "138/138 - 1s - loss: 0.0208 - auc: 0.8807 - val_loss: 0.0215 - val_auc: 0.8409\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02139\n",
      "138/138 - 1s - loss: 0.0208 - auc: 0.8836 - val_loss: 0.0215 - val_auc: 0.8329\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02139\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8887 - val_loss: 0.0215 - val_auc: 0.8410\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02139\n",
      "138/138 - 1s - loss: 0.0205 - auc: 0.8893 - val_loss: 0.0215 - val_auc: 0.8349\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02139\n",
      "138/138 - 1s - loss: 0.0204 - auc: 0.8914 - val_loss: 0.0215 - val_auc: 0.8313\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02139\n",
      "138/138 - 1s - loss: 0.0204 - auc: 0.8957 - val_loss: 0.0215 - val_auc: 0.8348\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.02139 to 0.02138, saving model to Best_weights_Fold:0.hdf5\n",
      "138/138 - 1s - loss: 0.0203 - auc: 0.8945 - val_loss: 0.0214 - val_auc: 0.8394\n",
      "#########################\n",
      "### FOLD 2\n",
      "###Training model 3l with seed 42\n",
      "### train on 17558 data validate on 4390 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21190, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.3698 - auc: 0.4969 - val_loss: 0.2119 - val_auc: 0.5216\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21190 to 0.19420, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.2781 - auc: 0.5498 - val_loss: 0.1942 - val_auc: 0.6458\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19420 to 0.10276, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.1780 - auc: 0.5790 - val_loss: 0.1028 - val_auc: 0.6587\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10276 to 0.03365, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0615 - auc: 0.6074 - val_loss: 0.0336 - val_auc: 0.7462\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03365 to 0.02616, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0306 - auc: 0.6880 - val_loss: 0.0262 - val_auc: 0.7632\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02616 to 0.02445, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0265 - auc: 0.7458 - val_loss: 0.0244 - val_auc: 0.7872\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02445 to 0.02357, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0250 - auc: 0.7706 - val_loss: 0.0236 - val_auc: 0.7891\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02357 to 0.02325, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0243 - auc: 0.7816 - val_loss: 0.0233 - val_auc: 0.7928\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02325 to 0.02302, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0240 - auc: 0.7883 - val_loss: 0.0230 - val_auc: 0.8064\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02302 to 0.02276, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0236 - auc: 0.7989 - val_loss: 0.0228 - val_auc: 0.8105\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02276 to 0.02257, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0233 - auc: 0.8064 - val_loss: 0.0226 - val_auc: 0.8135\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02257 to 0.02232, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0231 - auc: 0.8121 - val_loss: 0.0223 - val_auc: 0.8141\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02232 to 0.02224, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0229 - auc: 0.8196 - val_loss: 0.0222 - val_auc: 0.8137\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.02224\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8239 - val_loss: 0.0223 - val_auc: 0.8245\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02224\n",
      "138/138 - 1s - loss: 0.0225 - auc: 0.8308 - val_loss: 0.0223 - val_auc: 0.8219\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02224 to 0.02207, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8362 - val_loss: 0.0221 - val_auc: 0.8319\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02207 to 0.02190, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8392 - val_loss: 0.0219 - val_auc: 0.8300\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02190\n",
      "138/138 - 2s - loss: 0.0220 - auc: 0.8434 - val_loss: 0.0220 - val_auc: 0.8318\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02190 to 0.02188, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8461 - val_loss: 0.0219 - val_auc: 0.8299\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.02188\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8486 - val_loss: 0.0219 - val_auc: 0.8379\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02188 to 0.02176, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8554 - val_loss: 0.0218 - val_auc: 0.8412\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02176\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8569 - val_loss: 0.0219 - val_auc: 0.8377\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02176 to 0.02176, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8568 - val_loss: 0.0218 - val_auc: 0.8401\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02176 to 0.02166, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8616 - val_loss: 0.0217 - val_auc: 0.8337\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02166\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8655 - val_loss: 0.0218 - val_auc: 0.8349\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02166 to 0.02164, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8716 - val_loss: 0.0216 - val_auc: 0.8374\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02164 to 0.02154, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8776 - val_loss: 0.0215 - val_auc: 0.8395\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02154 to 0.02152, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8787 - val_loss: 0.0215 - val_auc: 0.8364\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.02152 to 0.02151, saving model to Best_weights_Fold:1.hdf5\n",
      "138/138 - 1s - loss: 0.0207 - auc: 0.8813 - val_loss: 0.0215 - val_auc: 0.8376\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.02151\n",
      "138/138 - 1s - loss: 0.0207 - auc: 0.8852 - val_loss: 0.0217 - val_auc: 0.8404\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02151\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8855 - val_loss: 0.0215 - val_auc: 0.8365\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02151\n",
      "138/138 - 1s - loss: 0.0205 - auc: 0.8875 - val_loss: 0.0216 - val_auc: 0.8299\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02151\n",
      "138/138 - 1s - loss: 0.0204 - auc: 0.8906 - val_loss: 0.0216 - val_auc: 0.8311\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02151\n",
      "138/138 - 1s - loss: 0.0203 - auc: 0.8933 - val_loss: 0.0216 - val_auc: 0.8347\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02151\n",
      "138/138 - 1s - loss: 0.0202 - auc: 0.8976 - val_loss: 0.0216 - val_auc: 0.8303\n",
      "#########################\n",
      "### FOLD 3\n",
      "###Training model 3l with seed 42\n",
      "### train on 17559 data validate on 4389 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21073, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.3701 - auc: 0.4993 - val_loss: 0.2107 - val_auc: 0.5282\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21073 to 0.20167, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.2786 - auc: 0.5504 - val_loss: 0.2017 - val_auc: 0.6152\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.20167 to 0.10416, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.1787 - auc: 0.5824 - val_loss: 0.1042 - val_auc: 0.6749\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10416 to 0.03340, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0624 - auc: 0.6129 - val_loss: 0.0334 - val_auc: 0.7552\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03340 to 0.02556, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0307 - auc: 0.6946 - val_loss: 0.0256 - val_auc: 0.7742\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02556 to 0.02386, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0264 - auc: 0.7476 - val_loss: 0.0239 - val_auc: 0.7921\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02386 to 0.02315, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0249 - auc: 0.7677 - val_loss: 0.0232 - val_auc: 0.7963\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02315 to 0.02289, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0243 - auc: 0.7830 - val_loss: 0.0229 - val_auc: 0.8032\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02289 to 0.02256, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0240 - auc: 0.7909 - val_loss: 0.0226 - val_auc: 0.8097\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02256 to 0.02230, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0236 - auc: 0.7992 - val_loss: 0.0223 - val_auc: 0.8187\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02230 to 0.02210, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0234 - auc: 0.8051 - val_loss: 0.0221 - val_auc: 0.8105\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.02210 to 0.02201, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0232 - auc: 0.8121 - val_loss: 0.0220 - val_auc: 0.8212\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02201 to 0.02196, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0229 - auc: 0.8172 - val_loss: 0.0220 - val_auc: 0.8209\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02196 to 0.02186, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0228 - auc: 0.8204 - val_loss: 0.0219 - val_auc: 0.8338\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02186 to 0.02177, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8301 - val_loss: 0.0218 - val_auc: 0.8303\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02177 to 0.02162, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8327 - val_loss: 0.0216 - val_auc: 0.8328\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02162 to 0.02151, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0223 - auc: 0.8394 - val_loss: 0.0215 - val_auc: 0.8301\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.02151\n",
      "138/138 - 1s - loss: 0.0222 - auc: 0.8402 - val_loss: 0.0216 - val_auc: 0.8340\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.02151 to 0.02150, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8441 - val_loss: 0.0215 - val_auc: 0.8448\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02150 to 0.02146, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0219 - auc: 0.8491 - val_loss: 0.0215 - val_auc: 0.8371\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02146 to 0.02137, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0218 - auc: 0.8529 - val_loss: 0.0214 - val_auc: 0.8382\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02137 to 0.02127, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8574 - val_loss: 0.0213 - val_auc: 0.8381\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.02127 to 0.02125, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8588 - val_loss: 0.0212 - val_auc: 0.8368\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02125\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8616 - val_loss: 0.0213 - val_auc: 0.8388\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.02125 to 0.02124, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 2s - loss: 0.0213 - auc: 0.8644 - val_loss: 0.0212 - val_auc: 0.8406\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02124 to 0.02116, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8689 - val_loss: 0.0212 - val_auc: 0.8384\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.02116\n",
      "138/138 - 1s - loss: 0.0212 - auc: 0.8734 - val_loss: 0.0213 - val_auc: 0.8481\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02116\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8739 - val_loss: 0.0212 - val_auc: 0.8383\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02116\n",
      "138/138 - 1s - loss: 0.0208 - auc: 0.8804 - val_loss: 0.0212 - val_auc: 0.8367\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.02116 to 0.02108, saving model to Best_weights_Fold:2.hdf5\n",
      "138/138 - 1s - loss: 0.0207 - auc: 0.8859 - val_loss: 0.0211 - val_auc: 0.8381\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02108\n",
      "138/138 - 1s - loss: 0.0207 - auc: 0.8841 - val_loss: 0.0211 - val_auc: 0.8375\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02108\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8878 - val_loss: 0.0212 - val_auc: 0.8421\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02108\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8930 - val_loss: 0.0213 - val_auc: 0.8489\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02108\n",
      "138/138 - 1s - loss: 0.0205 - auc: 0.8918 - val_loss: 0.0212 - val_auc: 0.8428\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02108\n",
      "138/138 - 1s - loss: 0.0203 - auc: 0.8968 - val_loss: 0.0211 - val_auc: 0.8401\n",
      "#########################\n",
      "### FOLD 4\n",
      "###Training model 3l with seed 42\n",
      "### train on 17558 data validate on 4390 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.20844, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 2s - loss: 0.3704 - auc: 0.5079 - val_loss: 0.2084 - val_auc: 0.5370\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.20844 to 0.19466, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.2810 - auc: 0.5571 - val_loss: 0.1947 - val_auc: 0.6440\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19466 to 0.10251, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.1778 - auc: 0.5792 - val_loss: 0.1025 - val_auc: 0.6711\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10251 to 0.03357, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0621 - auc: 0.6071 - val_loss: 0.0336 - val_auc: 0.7477\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03357 to 0.02571, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0306 - auc: 0.6911 - val_loss: 0.0257 - val_auc: 0.7729\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02571 to 0.02397, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0264 - auc: 0.7465 - val_loss: 0.0240 - val_auc: 0.7818\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02397 to 0.02333, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0251 - auc: 0.7666 - val_loss: 0.0233 - val_auc: 0.7857\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02333 to 0.02303, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0243 - auc: 0.7828 - val_loss: 0.0230 - val_auc: 0.7865\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02303 to 0.02270, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0240 - auc: 0.7901 - val_loss: 0.0227 - val_auc: 0.8072\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02270 to 0.02258, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0236 - auc: 0.7974 - val_loss: 0.0226 - val_auc: 0.8068\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02258 to 0.02237, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0233 - auc: 0.8050 - val_loss: 0.0224 - val_auc: 0.8147\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02237\n",
      "138/138 - 1s - loss: 0.0231 - auc: 0.8114 - val_loss: 0.0224 - val_auc: 0.8266\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02237 to 0.02218, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0229 - auc: 0.8185 - val_loss: 0.0222 - val_auc: 0.8240\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02218 to 0.02199, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8232 - val_loss: 0.0220 - val_auc: 0.8290\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.02199\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8276 - val_loss: 0.0220 - val_auc: 0.8299\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02199 to 0.02191, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8347 - val_loss: 0.0219 - val_auc: 0.8265\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02191 to 0.02181, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 2s - loss: 0.0222 - auc: 0.8357 - val_loss: 0.0218 - val_auc: 0.8316\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02181 to 0.02180, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8434 - val_loss: 0.0218 - val_auc: 0.8245\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02180\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8468 - val_loss: 0.0218 - val_auc: 0.8388\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02180 to 0.02173, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8494 - val_loss: 0.0217 - val_auc: 0.8362\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.02173 to 0.02161, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0217 - auc: 0.8545 - val_loss: 0.0216 - val_auc: 0.8334\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0217 - auc: 0.8559 - val_loss: 0.0217 - val_auc: 0.8437\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02161\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8595 - val_loss: 0.0216 - val_auc: 0.8384\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.02161 to 0.02157, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8634 - val_loss: 0.0216 - val_auc: 0.8355\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02157\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8631 - val_loss: 0.0216 - val_auc: 0.8374\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02157 to 0.02145, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8733 - val_loss: 0.0214 - val_auc: 0.8387\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02145 to 0.02140, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8750 - val_loss: 0.0214 - val_auc: 0.8418\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.02140\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8770 - val_loss: 0.0215 - val_auc: 0.8370\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02140\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8798 - val_loss: 0.0214 - val_auc: 0.8454\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.02140 to 0.02136, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0208 - auc: 0.8833 - val_loss: 0.0214 - val_auc: 0.8402\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.02136\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8875 - val_loss: 0.0214 - val_auc: 0.8423\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.02136 to 0.02131, saving model to Best_weights_Fold:3.hdf5\n",
      "138/138 - 1s - loss: 0.0205 - auc: 0.8894 - val_loss: 0.0213 - val_auc: 0.8399\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.02131\n",
      "138/138 - 1s - loss: 0.0205 - auc: 0.8910 - val_loss: 0.0213 - val_auc: 0.8397\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02131\n",
      "138/138 - 1s - loss: 0.0205 - auc: 0.8935 - val_loss: 0.0213 - val_auc: 0.8346\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02131\n",
      "138/138 - 1s - loss: 0.0203 - auc: 0.8938 - val_loss: 0.0214 - val_auc: 0.8379\n",
      "#########################\n",
      "### FOLD 5\n",
      "###Training model 3l with seed 42\n",
      "### train on 17559 data validate on 4389 data\n",
      "#########################\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-06.\n",
      "Epoch 1/35\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.21335, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.3707 - auc: 0.5023 - val_loss: 0.2134 - val_auc: 0.4954\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.00020080000000000003.\n",
      "Epoch 2/35\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.21335 to 0.19161, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.2796 - auc: 0.5510 - val_loss: 0.1916 - val_auc: 0.6363\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0004006000000000001.\n",
      "Epoch 3/35\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.19161 to 0.10228, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.1784 - auc: 0.5832 - val_loss: 0.1023 - val_auc: 0.6671\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.0006004000000000002.\n",
      "Epoch 4/35\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10228 to 0.03367, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0623 - auc: 0.6099 - val_loss: 0.0337 - val_auc: 0.7503\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0008002000000000002.\n",
      "Epoch 5/35\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03367 to 0.02620, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0307 - auc: 0.6963 - val_loss: 0.0262 - val_auc: 0.7618\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 6/35\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02620 to 0.02416, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0263 - auc: 0.7493 - val_loss: 0.0242 - val_auc: 0.7885\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 7/35\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02416 to 0.02363, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0249 - auc: 0.7701 - val_loss: 0.0236 - val_auc: 0.7900\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 8/35\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02363 to 0.02313, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0243 - auc: 0.7795 - val_loss: 0.0231 - val_auc: 0.8003\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 9/35\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02313 to 0.02289, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 2s - loss: 0.0240 - auc: 0.7906 - val_loss: 0.0229 - val_auc: 0.7996\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 10/35\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.02289 to 0.02285, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0236 - auc: 0.7985 - val_loss: 0.0228 - val_auc: 0.8127\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 11/35\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02285 to 0.02248, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0233 - auc: 0.8036 - val_loss: 0.0225 - val_auc: 0.8187\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 12/35\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.02248\n",
      "138/138 - 2s - loss: 0.0231 - auc: 0.8091 - val_loss: 0.0225 - val_auc: 0.8258\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 13/35\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02248 to 0.02229, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0229 - auc: 0.8164 - val_loss: 0.0223 - val_auc: 0.8183\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 14/35\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02229 to 0.02221, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8218 - val_loss: 0.0222 - val_auc: 0.8254\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 15/35\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.02221 to 0.02213, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0227 - auc: 0.8261 - val_loss: 0.0221 - val_auc: 0.8237\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 16/35\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.02213 to 0.02207, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0224 - auc: 0.8303 - val_loss: 0.0221 - val_auc: 0.8283\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 17/35\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02207 to 0.02199, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0223 - auc: 0.8356 - val_loss: 0.0220 - val_auc: 0.8281\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 18/35\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02199 to 0.02182, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0221 - auc: 0.8413 - val_loss: 0.0218 - val_auc: 0.8334\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 19/35\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.02182\n",
      "138/138 - 1s - loss: 0.0220 - auc: 0.8469 - val_loss: 0.0219 - val_auc: 0.8356\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 20/35\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.02182 to 0.02174, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0218 - auc: 0.8499 - val_loss: 0.0217 - val_auc: 0.8346\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 21/35\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.02174\n",
      "138/138 - 1s - loss: 0.0217 - auc: 0.8538 - val_loss: 0.0217 - val_auc: 0.8273\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 22/35\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.02174 to 0.02164, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0216 - auc: 0.8560 - val_loss: 0.0216 - val_auc: 0.8335\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 23/35\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0215 - auc: 0.8606 - val_loss: 0.0218 - val_auc: 0.8387\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 24/35\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0214 - auc: 0.8631 - val_loss: 0.0217 - val_auc: 0.8341\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00075.\n",
      "Epoch 25/35\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.02164\n",
      "138/138 - 1s - loss: 0.0213 - auc: 0.8668 - val_loss: 0.0217 - val_auc: 0.8355\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 26/35\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.02164 to 0.02151, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0211 - auc: 0.8716 - val_loss: 0.0215 - val_auc: 0.8375\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 27/35\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.02151 to 0.02151, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0210 - auc: 0.8758 - val_loss: 0.0215 - val_auc: 0.8420\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 28/35\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.02151 to 0.02150, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0209 - auc: 0.8789 - val_loss: 0.0215 - val_auc: 0.8413\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 29/35\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.02150\n",
      "138/138 - 1s - loss: 0.0208 - auc: 0.8812 - val_loss: 0.0215 - val_auc: 0.8443\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 30/35\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.02150 to 0.02149, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8856 - val_loss: 0.0215 - val_auc: 0.8487\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 31/35\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.02149 to 0.02147, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8866 - val_loss: 0.0215 - val_auc: 0.8412\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 32/35\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.02147\n",
      "138/138 - 1s - loss: 0.0206 - auc: 0.8904 - val_loss: 0.0215 - val_auc: 0.8348\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 33/35\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.02147 to 0.02143, saving model to Best_weights_Fold:4.hdf5\n",
      "138/138 - 1s - loss: 0.0204 - auc: 0.8920 - val_loss: 0.0214 - val_auc: 0.8360\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 34/35\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.02143\n",
      "138/138 - 1s - loss: 0.0203 - auc: 0.8933 - val_loss: 0.0214 - val_auc: 0.8344\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.0005625000000000001.\n",
      "Epoch 35/35\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.02143\n",
      "138/138 - 1s - loss: 0.0202 - auc: 0.8976 - val_loss: 0.0216 - val_auc: 0.8284\n",
      "FOLD mean log loss score is 0.01621838174992008\n",
      "Our final out of folds log loss for our blended models is 0.016067033921788987\n",
      "#########################\n",
      "#########################\n",
      "--- 8.275520090262095 minutes ---\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 5\n",
    "EPOCHS = 35\n",
    "#BatchSize = 128\n",
    "#VERBOSE = 2\n",
    "#SEEDS = 34\n",
    "SEEDS2 =  34 \n",
    "SEEDS3 =  42\n",
    "PATIENCE = 10\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#read the data and process them:\n",
    "train1 = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "test1 = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "\n",
    "\n",
    "#trn = pd.concat([train1.iloc[:,:4],train1[Top_feat]],  axis=1 )\n",
    "#test01 = pd.concat([test1.iloc[:,:4],test1[Top_feat]],  axis=1 )\n",
    "\n",
    "train01, train_targets, test12 = mapping_and_filter(train1, train_targets_scored, test1)\n",
    "\n",
    "#train, test = umap_embedding(train,test)\n",
    "#train, test = fe_pca(train01, test12, n_components_g = 70, n_components_c = 10, SEED = SEEDS)\n",
    "trn, tst = fct_stats(train01, test12)\n",
    "#train, test, features = scaling(train, test)\n",
    "\n",
    "#find the weight for initialise the model in using log(TP/TN)\n",
    "\n",
    "initial_bias, neg, pos = initiale_weights(train_targets)\n",
    "print('Targets Tp:\\n    Total: {}\\n    Positive: {} ({:.2f}% of total)\\n'.format((neg + pos), pos, 100 * pos / (neg + pos)))\n",
    "\n",
    "\n",
    "#print('#'*25)\n",
    "#print('#'*25)\n",
    "print('Weights initial biais:', initial_bias )\n",
    "#print(\"Training set:\",train.shape)\n",
    "#print(\"labels:\",train_targets.shape)\n",
    "#print(\"test:\",test.shape)\n",
    "#print(\"Features:\", features.shape)\n",
    "#print(\"submission:\",sample_submission.shape)\n",
    "\n",
    "#train the model\n",
    "#test_pred, oof_pred, history = train_model(train, train_targets, test, features, FOLDS, SEEDS, PATIENCE, VERBOSE, initial_bias)\n",
    "train41, test41 = fe_pca(trn, tst, n_components_g = 70, n_components_c = 10, SEED = SEEDS2)\n",
    "train41, test41, features41 = scaling(train41, test41)\n",
    "\n",
    "test_pred_4l, oof_pred_4l, history1 = train_model(train41, test41, train_targets, features41, FOLDS, SEED = SEEDS2, MODEL = '4l')\n",
    "\n",
    "train31, test31 = fe_pca(trn, tst, n_components_g = 70, n_components_c = 10, SEED = SEEDS3)\n",
    "train31, test31, features31 = scaling(train31, test31)\n",
    "\n",
    "test_pred_3l, oof_pred_3l, history2 = train_model(train31, test31, train_targets, features31, FOLDS, SEED = SEEDS3, MODEL = '3l')\n",
    "\n",
    "oof_pred = np.average([oof_pred_4l, oof_pred_3l], axis = 0)\n",
    "seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n",
    "print(f'Our final out of folds log loss for our blended models is {seed_log_loss}')\n",
    "\n",
    "test_pred = np.average([test_pred_4l, test_pred_3l], axis = 0)\n",
    "\n",
    "print('#'*25)\n",
    "print('#'*25)\n",
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.287797,
     "end_time": "2020-11-15T10:12:48.327599",
     "exception": false,
     "start_time": "2020-11-15T10:12:48.039802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Plot the LOSS and AUC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:12:48.921163Z",
     "iopub.status.busy": "2020-11-15T10:12:48.920007Z",
     "iopub.status.idle": "2020-11-15T10:12:49.625187Z",
     "shell.execute_reply": "2020-11-15T10:12:49.625710Z"
    },
    "papermill": {
     "duration": 1.008411,
     "end_time": "2020-11-15T10:12:49.625896",
     "exception": false,
     "start_time": "2020-11-15T10:12:48.617485",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.loglog([i for i in range(EPOCHS)], history1.history['loss'], 'g', label='Training loss')\n",
    "plt.loglog([i for i in range(EPOCHS)], history1.history['val_loss'], 'b', label='validation loss')\n",
    "   \n",
    "plt.title('Training and Validation loss MODEL 4 Layers ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:12:50.274449Z",
     "iopub.status.busy": "2020-11-15T10:12:50.273140Z",
     "iopub.status.idle": "2020-11-15T10:12:50.877932Z",
     "shell.execute_reply": "2020-11-15T10:12:50.878723Z"
    },
    "papermill": {
     "duration": 0.926683,
     "end_time": "2020-11-15T10:12:50.878944",
     "exception": false,
     "start_time": "2020-11-15T10:12:49.952261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog([i for i in range(EPOCHS)], history2.history['loss'], 'g', label='Training loss')\n",
    "plt.loglog([i for i in range(EPOCHS)], history2.history['val_loss'], 'b', label='validation loss')\n",
    "   \n",
    "plt.title('Training and Validation loss MODEL 3 Layers ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:12:51.706155Z",
     "iopub.status.busy": "2020-11-15T10:12:51.704511Z",
     "iopub.status.idle": "2020-11-15T10:12:51.901969Z",
     "shell.execute_reply": "2020-11-15T10:12:51.902560Z"
    },
    "papermill": {
     "duration": 0.524656,
     "end_time": "2020-11-15T10:12:51.902708",
     "exception": false,
     "start_time": "2020-11-15T10:12:51.378052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(EPOCHS)], history1.history['auc'], 'g', label='Training AUC')\n",
    "plt.plot([i for i in range(EPOCHS)], history1.history['val_auc'], 'b', label='validation auc')\n",
    "   \n",
    "plt.title('Training and Validation AUC MODEL 4 Layers ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:12:52.574438Z",
     "iopub.status.busy": "2020-11-15T10:12:52.573384Z",
     "iopub.status.idle": "2020-11-15T10:12:52.780712Z",
     "shell.execute_reply": "2020-11-15T10:12:52.781337Z"
    },
    "papermill": {
     "duration": 0.57318,
     "end_time": "2020-11-15T10:12:52.781548",
     "exception": false,
     "start_time": "2020-11-15T10:12:52.208368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([i for i in range(EPOCHS)], history2.history['auc'], 'g', label='Training AUC')\n",
    "plt.plot([i for i in range(EPOCHS)], history2.history['val_auc'], 'b', label='validation auc')\n",
    "   \n",
    "plt.title('Training and Validation AUC MODEL 3 Layers ')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:12:53.423470Z",
     "iopub.status.busy": "2020-11-15T10:12:53.422402Z",
     "iopub.status.idle": "2020-11-15T10:12:54.408571Z",
     "shell.execute_reply": "2020-11-15T10:12:54.409270Z"
    },
    "papermill": {
     "duration": 1.29663,
     "end_time": "2020-11-15T10:12:54.409448",
     "exception": false,
     "start_time": "2020-11-15T10:12:53.112818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final out of folds log loss for the models is 0.016067033921788987\n"
     ]
    }
   ],
   "source": [
    "seed_log_loss = mean_log_loss(train_targets.values, oof_pred)\n",
    "print(f'Final out of folds log loss for the models is {seed_log_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.300322,
     "end_time": "2020-11-15T10:12:55.008401",
     "exception": false,
     "start_time": "2020-11-15T10:12:54.708079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:12:55.618700Z",
     "iopub.status.busy": "2020-11-15T10:12:55.617782Z",
     "iopub.status.idle": "2020-11-15T10:12:55.622219Z",
     "shell.execute_reply": "2020-11-15T10:12:55.621607Z"
    },
    "papermill": {
     "duration": 0.309394,
     "end_time": "2020-11-15T10:12:55.622337",
     "exception": false,
     "start_time": "2020-11-15T10:12:55.312943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def submission(test_pred):\n",
    "    sample_submission.loc[:, train_targets.columns] = test_pred\n",
    "    sample_submission.loc[tst['cp_type'] == 1, train_targets.columns] = 0\n",
    "    sample_submission.to_csv('submission.csv', index = False)\n",
    "    return sample_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-15T10:12:56.226842Z",
     "iopub.status.busy": "2020-11-15T10:12:56.225746Z",
     "iopub.status.idle": "2020-11-15T10:12:59.254797Z",
     "shell.execute_reply": "2020-11-15T10:12:59.255334Z"
    },
    "papermill": {
     "duration": 3.334179,
     "end_time": "2020-11-15T10:12:59.255497",
     "exception": false,
     "start_time": "2020-11-15T10:12:55.921318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>...</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.002119</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.017914</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>0.004732</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.001936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.012590</td>\n",
       "      <td>0.015863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001273</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>0.010493</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.002193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.009411</td>\n",
       "      <td>0.004476</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.003164</td>\n",
       "      <td>0.022920</td>\n",
       "      <td>0.006974</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.004243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.002233</td>\n",
       "      <td>0.002205</td>\n",
       "      <td>0.018023</td>\n",
       "      <td>0.023945</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.001075</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.000359</td>\n",
       "      <td>0.001882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001083                0.002119   \n",
       "1  id_001897cda                     0.000391                0.001619   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.000912                0.001213   \n",
       "4  id_0027f1083                     0.003248                0.002233   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.002079                        0.017914   \n",
       "1        0.001543                        0.001404   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.002378                        0.010392   \n",
       "4        0.002205                        0.018023   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.024001                        0.004732   \n",
       "1                           0.002403                        0.003995   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.009411                        0.004476   \n",
       "4                           0.023945                        0.003975   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002010                       0.006878   \n",
       "1                    0.003875                       0.012590   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.002312                       0.004571   \n",
       "4                    0.006376                       0.002076   \n",
       "\n",
       "   adenylyl_cyclase_activator  ...  tropomyosin_receptor_kinase_inhibitor  \\\n",
       "0                    0.000313  ...                               0.001158   \n",
       "1                    0.015863  ...                               0.001273   \n",
       "2                    0.000000  ...                               0.000000   \n",
       "3                    0.000449  ...                               0.001018   \n",
       "4                    0.000510  ...                               0.001372   \n",
       "\n",
       "   trpv_agonist  trpv_antagonist  tubulin_inhibitor  \\\n",
       "0      0.002418         0.003063           0.000914   \n",
       "1      0.001160         0.002720           0.000171   \n",
       "2      0.000000         0.000000           0.000000   \n",
       "3      0.001010         0.003164           0.022920   \n",
       "4      0.000865         0.004631           0.002990   \n",
       "\n",
       "   tyrosine_kinase_inhibitor  ubiquitin_specific_protease_inhibitor  \\\n",
       "0                   0.000610                               0.001268   \n",
       "1                   0.010493                               0.001161   \n",
       "2                   0.000000                               0.000000   \n",
       "3                   0.006974                               0.000881   \n",
       "4                   0.001211                               0.001075   \n",
       "\n",
       "   vegfr_inhibitor  vitamin_b  vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0         0.000839   0.001868                    0.006079       0.001936  \n",
       "1         0.008219   0.000842                    0.003940       0.002193  \n",
       "2         0.000000   0.000000                    0.000000       0.000000  \n",
       "3         0.001325   0.002155                    0.001423       0.004243  \n",
       "4         0.001401   0.002041                    0.000359       0.001882  \n",
       "\n",
       "[5 rows x 207 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = submission(test_pred)\n",
    "sample_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 525.080564,
   "end_time": "2020-11-15T10:13:01.006538",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-15T10:04:15.925974",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
